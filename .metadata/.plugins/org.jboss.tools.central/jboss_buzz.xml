<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Understanding Red Hat AMQ Streams components for OpenShift and Kubernetes: Part 3</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/j3QGjMa_oUQ/" /><category term="Red Hat AMQ" /><category term="Red Hat Integration" /><category term="Red Hat OpenShift Container Platform" /><category term="Apache Kafka" /><category term="Red Hat AMQ Streams" /><author><name>Pramod Padmanabhan</name></author><id>https://developers.redhat.com/blog/?p=652107</id><updated>2019-12-06T08:00:14Z</updated><published>2019-12-06T08:00:14Z</published><content type="html">&lt;p&gt;In the previous articles in this series, we first covered the &lt;a href="https://developers.redhat.com/blog/2019/11/20/red-hat-amq-stre…ubernetes-part-1/"&gt;basics of Red Hat AMQ Streams on OpenShift&lt;/a&gt; and then showed &lt;a href="https://developers.redhat.com/blog/2019/11/20/red-hat-amq-stre…ubernetes-part-2/"&gt;how to set up Kafka Connect, a Kafka Bridge, and Kafka Mirror Maker.&lt;/a&gt; Here are a few key points to keep in mind before we proceed:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AMQ Streams is based on Apache Kafka.&lt;/li&gt; &lt;li&gt;AMQ Streams for the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.5/html/using_amq_streams_on_openshift_container_platform/" target="_blank" rel="noopener noreferrer"&gt;Red Hat OpenShift Container Platform&lt;/a&gt; is based on the Strimzi project.&lt;/li&gt; &lt;li&gt;AMQ Streams on containers has multiple components, such as the Cluster Operator, Entity Operator, Mirror Maker, Kafka connect, and Kafka Bridge.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Now that we have everything set up (or so we think), let&amp;#8217;s look at monitoring and alerting for our new environment.&lt;span id="more-652107"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;Kafka Exporter&lt;/h2&gt; &lt;p&gt;A Kafka cluster, by default, does not export all of its metrics. Hence, we need to use Kafka Exporter to collect the cluster&amp;#8217;s broker state, usage, and performance. It is important to have more insight so you can understand if the consumer&amp;#8217;s message consumption is at the same rate as the producer&amp;#8217;s message pushes. If not, this slow consumption behavior could cost the system. Catching these issues as early as possible is recommended.&lt;/p&gt; &lt;p&gt;To set up Kafka Exporter, begin by editing the existing Kafka cluster config, or creating a new one to include the Kafka Exporter. For example:&lt;/p&gt; &lt;pre&gt;apiVersion: kafka.strimzi.io/v1beta1 kind: Kafka metadata: name: simple-cluster spec: kafka: version: 2.3.0 replicas: 5 listeners: plain: {} tls: {} config: offsets.topic.replication.factor: 5 transaction.state.log.replication.factor: 5 transaction.state.log.min.isr: 2 log.message.format.version: "2.3" storage: type: jbod volumes: - id: 0 type: persistent-claim size: 5Gi deleteClaim: false zookeeper: replicas: 3 storage: type: persistent-claim size: 5Gi deleteClaim: false entityOperator: topicOperator: {} userOperator: {} kafkaExporter: {} &lt;/pre&gt; &lt;p&gt;Next, apply the new changes to the existing cluster:&lt;/p&gt; &lt;pre&gt;$ oc apply -f amq-kafka-cluster-kafka-exporter.yml&lt;/pre&gt; &lt;p&gt;You can see the result in Figure 1:&lt;/p&gt; &lt;div id="attachment_653737" style="width: 510px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dccd925de3ef.png"&gt;&lt;img aria-describedby="caption-attachment-653737" class="wp-image-653737" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dccd925de3ef-300x27.png" alt="Kafka Exporter is deployed." width="500" height="45" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dccd925de3ef-300x27.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dccd925de3ef.png 707w" sizes="(max-width: 500px) 100vw, 500px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-653737" class="wp-caption-text"&gt;Figure 1: Your new Kafka Exporter instance.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Prometheus and Grafana&lt;/h2&gt; &lt;p&gt;&lt;a href="https://prometheus.io/docs/introduction/overview/" target="_blank" rel="noopener noreferrer"&gt;Prometheus&lt;/a&gt; is a system monitoring and alerting toolkit that scrapes metrics from the Kafka cluster. The downside of this tool is that it does not have a good GUI. Hence, we create operational dashboards using &lt;a href="https://grafana.com/oss/grafana/" target="_blank" rel="noopener noreferrer"&gt;Grafana&lt;/a&gt; for the interface and Prometheus for the data feeds.&lt;/p&gt; &lt;p&gt;Let&amp;#8217;s get started with the metrics and creating the dashboard:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Add Kafka metrics to the Kafka resource. This snippet was referenced from the &lt;code&gt;examples/metrics/kafka-metrics.yaml&lt;/code&gt; file provided as part of the Red Hat AMQ Streams product:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;$ oc project amq-streams $ oc edit kafka simple-cluster # Add the below in the spec-&amp;#62;kafka metrics: # Inspired by config from Kafka 2.0.0 example rules: # https://github.com/prometheus/jmx_exporter/blob/master/example_configs/kafka-2_0_0.yml lowercaseOutputName: true rules: # Special cases and very specific rules - pattern : kafka.server&amp;#60;type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)&amp;#62;&amp;#60;&amp;#62;Value name: kafka_server_$1_$2 type: GAUGE labels: clientId: "$3" topic: "$4" partition: "$5" - pattern : kafka.server&amp;#60;type=(.+), name=(.+), clientId=(.+), brokerHost=(.+), brokerPort=(.+)&amp;#62;&amp;#60;&amp;#62;Value name: kafka_server_$1_$2 type: GAUGE labels: clientId: "$3" broker: "$4:$5" # Some percent metrics use MeanRate attribute # Ex) kafka.server&amp;#60;type=(KafkaRequestHandlerPool), name=(RequestHandlerAvgIdlePercent)&amp;#62;&amp;#60;&amp;#62;MeanRate - pattern: kafka.(\w+)&amp;#60;type=(.+), name=(.+)Percent\w*&amp;#62;&amp;#60;&amp;#62;MeanRate name: kafka_$1_$2_$3_percent type: GAUGE # Generic gauges for percents - pattern: kafka.(\w+)&amp;#60;type=(.+), name=(.+)Percent\w*&amp;#62;&amp;#60;&amp;#62;Value name: kafka_$1_$2_$3_percent type: GAUGE - pattern: kafka.(\w+)&amp;#60;type=(.+), name=(.+)Percent\w*, (.+)=(.+)&amp;#62;&amp;#60;&amp;#62;Value name: kafka_$1_$2_$3_percent type: GAUGE labels: "$4": "$5" # Generic per-second counters with 0-2 key/value pairs - pattern: kafka.(\w+)&amp;#60;type=(.+), name=(.+)PerSec\w*, (.+)=(.+), (.+)=(.+)&amp;#62;&amp;#60;&amp;#62;Count name: kafka_$1_$2_$3_total type: COUNTER labels: "$4": "$5" "$6": "$7" - pattern: kafka.(\w+)&amp;#60;type=(.+), name=(.+)PerSec\w*, (.+)=(.+)&amp;#62;&amp;#60;&amp;#62;Count name: kafka_$1_$2_$3_total type: COUNTER labels: "$4": "$5" - pattern: kafka.(\w+)&amp;#60;type=(.+), name=(.+)PerSec\w*&amp;#62;&amp;#60;&amp;#62;Count name: kafka_$1_$2_$3_total type: COUNTER # Generic gauges with 0-2 key/value pairs - pattern: kafka.(\w+)&amp;#60;type=(.+), name=(.+), (.+)=(.+), (.+)=(.+)&amp;#62;&amp;#60;&amp;#62;Value name: kafka_$1_$2_$3 type: GAUGE labels: "$4": "$5" "$6": "$7" - pattern: kafka.(\w+)&amp;#60;type=(.+), name=(.+), (.+)=(.+)&amp;#62;&amp;#60;&amp;#62;Value name: kafka_$1_$2_$3 type: GAUGE labels: "$4": "$5" - pattern: kafka.(\w+)&amp;#60;type=(.+), name=(.+)&amp;#62;&amp;#60;&amp;#62;Value name: kafka_$1_$2_$3 type: GAUGE # Emulate Prometheus 'Summary' metrics for the exported 'Histogram's. # Note that these are missing the '_sum' metric! - pattern: kafka.(\w+)&amp;#60;type=(.+), name=(.+), (.+)=(.+), (.+)=(.+)&amp;#62;&amp;#60;&amp;#62;Count name: kafka_$1_$2_$3_count type: COUNTER labels: "$4": "$5" "$6": "$7" - pattern: kafka.(\w+)&amp;#60;type=(.+), name=(.+), (.+)=(.*), (.+)=(.+)&amp;#62;&amp;#60;&amp;#62;(\d+)thPercentile name: kafka_$1_$2_$3 type: GAUGE labels: "$4": "$5" "$6": "$7" quantile: "0.$8" - pattern: kafka.(\w+)&amp;#60;type=(.+), name=(.+), (.+)=(.+)&amp;#62;&amp;#60;&amp;#62;Count name: kafka_$1_$2_$3_count type: COUNTER labels: "$4": "$5" - pattern: kafka.(\w+)&amp;#60;type=(.+), name=(.+), (.+)=(.*)&amp;#62;&amp;#60;&amp;#62;(\d+)thPercentile name: kafka_$1_$2_$3 type: GAUGE labels: "$4": "$5" quantile: "0.$6" - pattern: kafka.(\w+)&amp;#60;type=(.+), name=(.+)&amp;#62;&amp;#60;&amp;#62;Count name: kafka_$1_$2_$3_count type: COUNTER - pattern: kafka.(\w+)&amp;#60;type=(.+), name=(.+)&amp;#62;&amp;#60;&amp;#62;(\d+)thPercentile name: kafka_$1_$2_$3 type: GAUGE labels: quantile: "0.$4" &lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;This process will restart the &lt;code&gt;simple-cluster-kafka&lt;/code&gt; pods one by one, as shown in Figure 2:&lt;/p&gt; &lt;div id="attachment_654297" style="width: 510px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcdd6da69fc4.png"&gt;&lt;img aria-describedby="caption-attachment-654297" class="wp-image-654297" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcdd6da69fc4-300x106.png" alt="Restarting the pods." width="500" height="177" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcdd6da69fc4-300x106.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcdd6da69fc4.png 674w" sizes="(max-width: 500px) 100vw, 500px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-654297" class="wp-caption-text"&gt;Figure 2: Restarting the pods.&lt;/p&gt;&lt;/div&gt; &lt;ol start="2"&gt; &lt;li&gt;Confirm you have Prometheus running in the cluster:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;$ oc get pod -n openshift-monitoring | grep prometheus prometheus-k8s-0 4/4 Running 140 3d prometheus-k8s-1 4/4 Running 140 3d prometheus-operator-687784bd4b-56vsk 1/1 Running 127 3d&lt;/pre&gt; &lt;p&gt;In case the above does not return any Prometheus pods, check with your infrastructure team to know where they are installed. If they are not installed, then check the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.5/html/using_amq_streams_on_openshift/assembly-metrics-setup-str#assembly-metrics-prometheus-str" target="_blank" rel="noopener noreferrer"&gt;Prometheus installation steps from the docs&lt;/a&gt;.&lt;/p&gt; &lt;ol start="3"&gt; &lt;li&gt;Install Grafana:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;$ oc create -f https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/metrics/examples/grafana/grafana.yaml&lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;You can see the results in Figure 3:&lt;/p&gt; &lt;div id="attachment_654307" style="width: 510px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcde27326144.png"&gt;&lt;img aria-describedby="caption-attachment-654307" class="wp-image-654307" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcde27326144-300x36.png" alt="Graphana is deployed." width="500" height="60" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcde27326144-300x36.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcde27326144.png 704w" sizes="(max-width: 500px) 100vw, 500px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-654307" class="wp-caption-text"&gt;Figure 3: Grafana is now deployed.&lt;/p&gt;&lt;/div&gt; &lt;ol start="4"&gt; &lt;li&gt;Create a route for the Grafana service:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;$ oc expose svc grafana --name=grafana-route&lt;/pre&gt; &lt;ol start="5"&gt; &lt;li&gt;Log into the admin console (shown in Figure 4):&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;$ oc get route | grep grafana grafana-route grafana-route-amq-streams.apps.redhat.demo.com grafana&lt;/pre&gt; &lt;div id="attachment_654317" style="width: 510px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcde3b42e910.png"&gt;&lt;img aria-describedby="caption-attachment-654317" class="wp-image-654317" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcde3b42e910-300x130.png" alt="The Grafana admin window." width="500" height="216" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcde3b42e910-300x130.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcde3b42e910.png 762w" sizes="(max-width: 500px) 100vw, 500px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-654317" class="wp-caption-text"&gt;Figure 4: The Grafana admin window.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The default credentials are &lt;code&gt;admin&lt;/code&gt; and &lt;code&gt;admin&lt;/code&gt;.&lt;/p&gt; &lt;p style="padding-left: 40px;"&gt;When you log in for the first time, Grafana will ask you to change the password.&lt;/p&gt; &lt;ol start="6"&gt; &lt;li&gt;Add Prometheus as a data source. Go to the settings icon and select &lt;em&gt;Data Sources&lt;/em&gt;, and then &lt;em&gt;Prometheus,&lt;/em&gt; as shown in Figure 5:&lt;/li&gt; &lt;/ol&gt; &lt;div id="attachment_654337" style="width: 217px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcde684b32dd.png"&gt;&lt;img aria-describedby="caption-attachment-654337" class="wp-image-654337 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcde684b32dd.png" alt="Graphana settings." width="207" height="205" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcde684b32dd.png 207w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcde684b32dd-150x150.png 150w" sizes="(max-width: 207px) 100vw, 207px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-654337" class="wp-caption-text"&gt;Figure 5: The Graphana settings menu.&lt;/p&gt;&lt;/div&gt; &lt;ol start="7"&gt; &lt;li&gt;Enter the Prometheus details and save them:&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;URL:&lt;/strong&gt; https://prometheus-k8s.openshift-monitoring.svc:9091&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Basic Auth:&lt;/strong&gt; Enabled&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Skip TLS Verify:&lt;/strong&gt; Enabled&lt;/li&gt; &lt;li&gt;&lt;strong&gt;User:&lt;/strong&gt; Internal&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pass:&lt;/strong&gt; Get these details from the cluster admin.&lt;/li&gt; &lt;/ul&gt; &lt;p style="padding-left: 40px;"&gt;The message &amp;#8220;Data source is working&amp;#8221; should appear at the bottom before you continue, as shown in Figure 6:&lt;/p&gt; &lt;div id="attachment_654347" style="width: 310px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcdec58e1277.png"&gt;&lt;img aria-describedby="caption-attachment-654347" class="wp-image-654347" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcdec58e1277-253x300.png" alt="Configuring Graphana." width="300" height="356" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcdec58e1277-253x300.png 253w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcdec58e1277.png 766w" sizes="(max-width: 300px) 100vw, 300px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-654347" class="wp-caption-text"&gt;Figure 6: Ready to proceed with the new configuration.&lt;/p&gt;&lt;/div&gt; &lt;ol start="8"&gt; &lt;li&gt;Open the &lt;em&gt;Import&lt;/em&gt; options by selecting &lt;strong&gt;+&lt;/strong&gt; and then &lt;em&gt;Import&lt;/em&gt;, as shown in Figure 7:&lt;/li&gt; &lt;/ol&gt; &lt;div id="attachment_654327" style="width: 229px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcde5947757f.png"&gt;&lt;img aria-describedby="caption-attachment-654327" class="wp-image-654327 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcde5947757f.png" alt="Open the import menu to import a sample dashboard." width="219" height="241" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-654327" class="wp-caption-text"&gt;Figure 7: Select + and then Import to access the Import options dialog box.&lt;/p&gt;&lt;/div&gt; &lt;ol start="9"&gt; &lt;li&gt;Click &lt;em&gt;Upload .json file&lt;/em&gt; to add the&lt;code&gt; strimzi-kafka.json&lt;/code&gt;file from &lt;code&gt;examples/metrics/grafana-dashboards/strimzi-kafka.json&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Select &lt;em&gt;Prometheus &lt;/em&gt;as a data source and then click &lt;em&gt;Import:&lt;/em&gt;&lt;/li&gt; &lt;/ol&gt; &lt;div id="attachment_654357" style="width: 510px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcded448a351.png"&gt;&lt;img aria-describedby="caption-attachment-654357" class="wp-image-654357" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcded448a351-300x106.png" alt="Importing the sample dashboard." width="500" height="177" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcded448a351-300x106.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcded448a351-768x272.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcded448a351.png 871w" sizes="(max-width: 500px) 100vw, 500px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-654357" class="wp-caption-text"&gt;Figure 8: Importing the sample dashboard.&lt;/p&gt;&lt;/div&gt; &lt;p style="padding-left: 40px;"&gt;Doing this should result in the sample Grafana dashboard.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this article, we explored tools that can help us monitor Red Hat AMQ Streams. This piece fully rounds out our series, where we covered:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2019/11/20/red-hat-amq-stre%E2%80%A6ubernetes-part-1/"&gt;Zookeeper, Kafka, and Entity Operator creation.&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2019/11/20/red-hat-amq-stre%E2%80%A6ubernetes-part-2/"&gt;Kafka Connect, Kafka Bridge, and Mirror Maker.&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Monitoring and admin (this article).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Hopefully, you now have a better understanding of how to run AMQ Streams in a container ecosystem. Work through our examples and see the results for yourself.&lt;/p&gt; &lt;h3&gt;References&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://docs.confluent.io/current/connect/index.html" target="_blank" rel="noopener noreferrer"&gt;Kafka Connect documentation&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.5/html-single/using_amq_streams_on_openshift/index" target="_blank" rel="noopener noreferrer"&gt;Using AMQ Streams on OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://strimzi.io/2019/10/14/improving-prometheus-metrics.html" target="_blank" rel="noopener noreferrer"&gt;Improving Prometheus metrics&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F06%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-3%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%203" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F06%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-3%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%203" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F06%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-3%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%203" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F06%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-3%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%203" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F06%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-3%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%203" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F06%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-3%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%203" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F06%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-3%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%203" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F06%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-3%2F&amp;#038;title=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%203" data-a2a-url="https://developers.redhat.com/blog/2019/12/06/understanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-3/" data-a2a-title="Understanding Red Hat AMQ Streams components for OpenShift and Kubernetes: Part 3"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/12/06/understanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-3/"&gt;Understanding Red Hat AMQ Streams components for OpenShift and Kubernetes: Part 3&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/j3QGjMa_oUQ" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;In the previous articles in this series, we first covered the basics of Red Hat AMQ Streams on OpenShift and then showed how to set up Kafka Connect, a Kafka Bridge, and Kafka Mirror Maker. Here are a few key points to keep in mind before we proceed: AMQ Streams is based on Apache Kafka. AMQ [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/12/06/understanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-3/"&gt;Understanding Red Hat AMQ Streams components for OpenShift and Kubernetes: Part 3&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><post-id xmlns="com-wordpress:feed-additions:1">652107</post-id><dc:creator>Pramod Padmanabhan</dc:creator><dc:date>2019-12-06T08:00:14Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/12/06/understanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-3/</feedburner:origLink></entry><entry><title>Introduction to the Red Hat OpenShift deployment extension for Microsoft Azure DevOps</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/KOJekK8uNAg/" /><category term="Developer Tools" /><category term="OpenShift" /><category term="OpenShift VSTS extension" /><author><name>Luca Stocchi</name></author><id>https://developers.redhat.com/blog/?p=650877</id><updated>2019-12-05T08:00:11Z</updated><published>2019-12-05T08:00:11Z</published><content type="html">&lt;p&gt;We are extremely pleased to present the new version of the &lt;a href="https://developers.redhat.com/openshift/"&gt;Red Hat OpenShift&lt;/a&gt; deployment extension (OpenShift VSTS) 1.4.0 for Microsoft Azure DevOps. This extension enables users to deploy their applications to any OpenShift cluster directly from their Microsoft Azure DevOps account. In this article, we will look at how to install and use this extension as part of a YAML-defined pipeline with both Microsoft-hosted and self-hosted agents.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The OpenShift VSTS extension can be downloaded directly from the marketplace at this &lt;a href="https://marketplace.visualstudio.com/items?itemName=redhat.openshift-vsts" target="_blank" rel="noopener noreferrer"&gt;link&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This article offers a demonstration where we explain how easy it is to set up everything and start working with the extension. Look at the &lt;a href="https://github.com/redhat-developer/openshift-vsts/blob/master/docs/getting-started.md" target="_blank" rel="noopener noreferrer"&gt;README file&lt;/a&gt; for further installation and usage information.&lt;span id="more-650877"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;&lt;iframe src="https://www.youtube.com/embed/RBwpedmkvow" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"&gt;&lt;/iframe&gt;&lt;/p&gt; &lt;h2&gt;The benefits&lt;/h2&gt; &lt;p&gt;The new OpenShift VSTS 1.4.0 extension has three major benefits:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;It allows users to use an &lt;code&gt;oc&lt;/code&gt; CLI already installed on their machine when using a local agent.&lt;/li&gt; &lt;li&gt;It supports and automatically downloads &lt;code&gt;oc&lt;/code&gt; versions greater than four.&lt;/li&gt; &lt;li&gt;It changes the way the &lt;code&gt;oc&lt;/code&gt; CLI is downloaded: No more &amp;#8220;API rate limit exceeded&amp;#8221; error from the GitHub REST API.&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Installing the OpenShift VSTS extension&lt;/h2&gt; &lt;p&gt;Before to start using the OpenShift VSTS extension, you first need a running OpenShift instance. In our demo video, we use OpenShift Online, which is hosted and managed by Red Hat. You can &lt;a href="https://www.openshift.com/trial/" target="_blank" rel="noopener noreferrer"&gt;sign up here&lt;/a&gt; and start using OpenShift in the cloud for free.&lt;/p&gt; &lt;p&gt;You also need a Microsoft Azure DevOps account. Once you log into this account, you should see a list of your organizations on the left, and all projects related to your organization on the right. If you do not have any projects, it is time to add a new one. To do so, follow these steps:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Clicking on &lt;em&gt;New Project&lt;/em&gt; and fill in the required fields, as shown in Figure 1:&lt;/li&gt; &lt;/ol&gt; &lt;div id="attachment_651117" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-651117" class="wp-image-651117 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/azure-devops-home-1024x409.png" alt="" width="640" height="256" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/azure-devops-home.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/azure-devops-home-300x120.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/azure-devops-home-768x307.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;p id="caption-attachment-651117" class="wp-caption-text"&gt;Figure 1: Creating a new Microsoft Azure DevOps project.&lt;/p&gt;&lt;/div&gt; &lt;ol start="2"&gt; &lt;li&gt;Go to &lt;a href="https://marketplace.visualstudio.com/items?itemName=redhat.openshift-vsts" target="_blank" rel="noopener noreferrer"&gt;https://marketplace.visualstudio.com/items?itemName=redhat.openshift-vsts&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Click on &lt;em&gt;Get it free.&lt;/em&gt;&lt;/li&gt; &lt;li&gt;Select your Azure DevOps organization and click &lt;em&gt;Install&lt;/em&gt;. Once this process finishes, the OpenShift VSTS extension install is complete, and you can start setting up your account.&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Connecting to your OpenShift cluster&lt;/h2&gt; &lt;p&gt;Now, you need to configure the OpenShift service connection, which connects Microsoft Azure DevOps to your OpenShift cluster:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Log into your Azure DevOps project.&lt;/li&gt; &lt;li&gt;Click on &lt;em&gt;Project Settings&lt;/em&gt; (the cogwheel icon) on the page&amp;#8217;s bottom left.&lt;/li&gt; &lt;li&gt;Select &lt;em&gt;Service Connections&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;Click on &lt;em&gt;New service connection&lt;/em&gt; and search for OpenShift.&lt;/li&gt; &lt;li&gt;Pick the authentication method you would like to use (basic, token, or &lt;code&gt;kubeconfig&lt;/code&gt;). See the details for each option in the next few sections.&lt;/li&gt; &lt;li&gt;Insert your own OpenShift cluster data.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Congratulations! You have connected your Azure DevOps account to your OpenShift cluster.&lt;/p&gt; &lt;p&gt;Now, let&amp;#8217;s look at how to set up each authentication method.&lt;/p&gt; &lt;h3&gt;Basic authentication&lt;/h3&gt; &lt;p&gt;When you select &lt;em&gt;Basic Authentication&lt;/em&gt;, use the following information to fill out the dialog:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Connection Name:&lt;/strong&gt; The name you will use to refer to this service connection.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Server URL:&lt;/strong&gt; The OpenShift cluster&amp;#8217;s URL.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Username:&lt;/strong&gt; The OpenShift username for this instance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Password:&lt;/strong&gt; The password for the specified user.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Accept untrusted SSL certificates:&lt;/strong&gt; Whether it is ok to accept self-signed (untrusted) certificates.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Allow all pipelines to use this connection:&lt;/strong&gt; Allows YAML-defined pipelines to use our service connection (they are not automatically authorized for service connections).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The result should look similar to Figure 2:&lt;/p&gt; &lt;div id="attachment_651127" style="width: 638px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-651127" class="wp-image-651127 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/basic-authentication-form.png" alt="" width="628" height="457" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/basic-authentication-form.png 628w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/basic-authentication-form-300x218.png 300w" sizes="(max-width: 628px) 100vw, 628px" /&gt;&lt;p id="caption-attachment-651127" class="wp-caption-text"&gt;Figure 2: Using basic authentication with an OpenShift service connection.&lt;/p&gt;&lt;/div&gt; &lt;h3&gt;Token authentication&lt;/h3&gt; &lt;p&gt;When you select &lt;em&gt;Token Based Authentication&lt;/em&gt;, use the following information to fill out the dialog:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Connection Name:&lt;/strong&gt; The name you will use to refer to this service connection.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Server URL:&lt;/strong&gt; The OpenShift cluster&amp;#8217;s URL.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Accept untrusted SSL certificates:&lt;/strong&gt; Whether it is ok to accept self-signed (untrusted) certificates.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;API Token:&lt;/strong&gt; The API token used for authentication.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Allow all pipelines to use this connection:&lt;/strong&gt; Allows YAML-defined pipelines to use our service connection (they are not automatically authorized for service connections).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The result should look similar to Figure 3:&lt;/p&gt; &lt;div id="attachment_651187" style="width: 645px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-651187" class="wp-image-651187 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/token-authetication-form.png" alt="" width="635" height="422" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/token-authetication-form.png 635w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/token-authetication-form-300x199.png 300w" sizes="(max-width: 635px) 100vw, 635px" /&gt;&lt;p id="caption-attachment-651187" class="wp-caption-text"&gt;Figure 3: Using token authentication with an OpenShift service connection.&lt;/p&gt;&lt;/div&gt; &lt;h3&gt;Kubeconfig&lt;/h3&gt; &lt;p&gt;To use &lt;code&gt;kubeconfig&lt;/code&gt;-based authentication, select &lt;em&gt;No Authentication &lt;/em&gt;and use the following information to fill out the dialog:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Connection Name:&lt;/strong&gt; The name you will use to refer to this service connection.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Server URL:&lt;/strong&gt; The OpenShift cluster&amp;#8217;s URL.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Kubeconfig:&lt;/strong&gt; The contents of the &lt;code&gt;kubectl&lt;/code&gt; configuration file.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Allow all pipelines to use this connection:&lt;/strong&gt; Allows YAML-defined pipelines to use our service connection (they are not automatically authorized for service connections).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The result should look similar to Figure 4:&lt;/p&gt; &lt;div id="attachment_651177" style="width: 646px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-651177" class="wp-image-651177 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/kubectl-authentication-form.png" alt="" width="636" height="397" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/kubectl-authentication-form.png 636w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/kubectl-authentication-form-300x187.png 300w" sizes="(max-width: 636px) 100vw, 636px" /&gt;&lt;p id="caption-attachment-651177" class="wp-caption-text"&gt;Figure 4: Using kubeconfig authentication with an OpenShift service connection.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Exploring the extension&lt;/h2&gt; &lt;p&gt;Once the extension can authenticate to the Red Hat OpenShift cluster, you are ready to create your own YAML pipeline, and then perform operations in OpenShift by executing &lt;code&gt;oc&lt;/code&gt; commands directly from Azure DevOps.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This extension uses the &lt;code&gt;oc&lt;/code&gt; OpenShift client tool to interact with an OpenShift cluster, so a minimal knowledge of this OpenShift CLI tool is required.&lt;/p&gt; &lt;p&gt;The extension offers three different tasks: install and set up &lt;code&gt;oc&lt;/code&gt;, execute a single &lt;code&gt;oc&lt;/code&gt; command, and update the &lt;code&gt;ConfigMap&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Install and set up &lt;code&gt;oc&lt;/code&gt;&lt;/h3&gt; &lt;p&gt;This task allows you to install a specific version of the OpenShift CLI (&lt;code&gt;oc&lt;/code&gt;), adds it to your &lt;code&gt;PATH&lt;/code&gt;, and creates a &lt;code&gt;kubeconfig&lt;/code&gt; file for authenticating with the OpenShift cluster. First, we download and set up &lt;code&gt;oc&lt;/code&gt;, and then we execute &lt;code&gt;oc&lt;/code&gt; commands through a script:&lt;/p&gt; &lt;pre&gt;jobs: - job: myjob   displayName: MyJob   pool:     vmImage: 'windows-latest'   steps:   # Install oc so that it can be used within a 'script' or bash 'task'   - task: oc-setup@2     inputs:       openshiftService: 'My Openshift'       version: '3.11.154' # A script task making use of 'oc'   - script: |       oc new-project my-project       oc apply -f ${SYSTEM_DEFAULTWORKINGDIRECTORY}/openshift/config.yaml -n my-project&lt;/pre&gt; &lt;p&gt;The installed &lt;code&gt;oc&lt;/code&gt; binary will match your agent&amp;#8217;s OS.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; It is possible to use variables defined in the agent. As seen in this example, to reference a file in &lt;code&gt;artefact _my_sources&lt;/code&gt;, you can use:&lt;/p&gt; &lt;pre style="padding-left: 40px;"&gt;${SYSTEM_DEFAULTWORKINGDIRECTORY}/_my_sources/my-openshift-config.yaml&lt;/pre&gt; &lt;p&gt;You can use this task as follows in the GUI:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;In &lt;em&gt;Tasks&lt;/em&gt;, click &lt;em&gt;Install and setup oc&lt;/em&gt;. This action opens the dialog shown in Figure 5:&lt;/li&gt; &lt;/ol&gt; &lt;div id="attachment_651167" style="width: 510px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-651167" class="wp-image-651167" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/install-oc-task-300x113.png" alt="" width="500" height="188" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/install-oc-task-300x113.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/install-oc-task-768x289.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/install-oc-task.png 797w" sizes="(max-width: 500px) 100vw, 500px" /&gt;&lt;p id="caption-attachment-651167" class="wp-caption-text"&gt;Figure 5: Installing and setting up oc.&lt;/p&gt;&lt;/div&gt; &lt;ol start="2"&gt; &lt;li&gt;In the &lt;em&gt;OpenShift service connection &lt;/em&gt;drop-down box, select the service connection you just created, which will be used to execute this command.&lt;/li&gt; &lt;li&gt;In the &lt;em&gt;Version of oc to use&lt;/em&gt; text box, add the version of &lt;code&gt;oc&lt;/code&gt; you want to use (e.g., 3.11.154) or a direct URL to an &lt;code&gt;oc&lt;/code&gt; release bundle. (If left blank, the latest stable oc version is used.)&lt;/li&gt; &lt;/ol&gt; &lt;h3&gt;Execute single &lt;code&gt;oc&lt;/code&gt; commands&lt;/h3&gt; &lt;p&gt;This task allows you to execute a single &lt;code&gt;oc&lt;/code&gt; command directly from Azure DevOps:&lt;/p&gt; &lt;pre&gt;jobs: - job: myjob   displayName: MyJob   pool:     name: 'Default'   steps:   - task: oc-cmd@2     inputs:       openshiftService: 'My Openshift'       version: '4.1'       cmd: 'oc new-app https://github.com/lstocchi/nodejs-ex -l name=demoapp'       uselocalOc: true&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Neither the &lt;code&gt;oc-cmd&lt;/code&gt; or &lt;code&gt;config-map&lt;/code&gt; tasks need to forcibly run after the setup task. If the extension does not find a valid &lt;code&gt;oc&lt;/code&gt; CLI during the execution of an &lt;code&gt;oc&lt;/code&gt; command, first it downloads a copy of a new &lt;code&gt;oc&lt;/code&gt;, and then it executes the command.&lt;/p&gt; &lt;p&gt;To use this task in the GUI:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;In &lt;em&gt;Tasks,&lt;/em&gt; select &lt;em&gt;Execute oc command &lt;/em&gt;to pull up the dialog shown in Figure 6:&lt;/li&gt; &lt;/ol&gt; &lt;div id="attachment_651147" style="width: 510px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-651147" class="wp-image-651147" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/execute-oc-task-300x178.png" alt="" width="500" height="297" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/execute-oc-task-300x178.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/execute-oc-task.png 679w" sizes="(max-width: 500px) 100vw, 500px" /&gt;&lt;p id="caption-attachment-651147" class="wp-caption-text"&gt;Figure 6: Fill out this dialog to execute an oc command.&lt;/p&gt;&lt;/div&gt; &lt;ol start="2"&gt; &lt;li&gt;In the &lt;em&gt;OpenShift service connection&lt;/em&gt; drop-down box, select the service connection you just created, which will be used to execute this command.&lt;/li&gt; &lt;li&gt;In the &lt;em&gt;Version of oc to use&lt;/em&gt; text box, add the version of &lt;code&gt;oc&lt;/code&gt; you want to use (e.g., 3.11.154) or a direct URL to an &lt;code&gt;oc&lt;/code&gt; release bundle. (If left blank, the latest stable oc version is used.)&lt;/li&gt; &lt;li&gt;In the &lt;em&gt;Command to run&lt;/em&gt; text box, enter the actual &lt;code&gt;oc&lt;/code&gt; command to run.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; You can directly type the &lt;code&gt;oc&lt;/code&gt; sub-command by omitting &lt;code&gt;oc&lt;/code&gt; from the input (e.g., &lt;code&gt;rollout latest dc/my-app -n production&lt;/code&gt;).&lt;/p&gt; &lt;ol start="5"&gt; &lt;li&gt;Check or un-check the &lt;em&gt;Ignore non success return value&lt;/em&gt; check box, which specifies whether the &lt;code&gt;oc&lt;/code&gt;command&amp;#8217;s non-success return value has to be ignored (e.g., if a task with the command &lt;code&gt;oc create&lt;/code&gt; or &lt;code&gt;oc delete&lt;/code&gt;fails because the resource has already been created or deleted, the pipeline will continue its execution).&lt;/li&gt; &lt;li&gt;Check or un-check the &lt;em&gt;use local oc executable&lt;/em&gt; check box, which specified whether to force the extension to use, if present, the &lt;code&gt;oc&lt;/code&gt; CLI found on the machine containing the agent. &lt;em&gt;If no version is specified&lt;/em&gt;, the extension uses the local &lt;code&gt;oc&lt;/code&gt; CLI no matter what its version is. &lt;em&gt;If a version is specified&lt;/em&gt;, then the extension checks to see if the &lt;code&gt;oc&lt;/code&gt; CLI installed has the same version requested by the user (if not, the correct &lt;code&gt;oc&lt;/code&gt; CLI will be downloaded).&lt;/li&gt; &lt;/ol&gt; &lt;h3&gt;Update a &lt;code&gt;ConfigMap&lt;/code&gt;&lt;/h3&gt; &lt;p&gt;This task allows you to update the properties of a given &lt;code&gt;ConfigMap&lt;/code&gt; using a grid:&lt;/p&gt; &lt;pre&gt;  jobs: - job: myjob   displayName: MyJob   pool:     name: 'Default' - task: config-map@2      inputs:        openshiftService: 'my_openshift_connection'        configMapName: 'my-config'        namespace: 'my-project'        properties: '-my-key1 my-value1 -my-key2 my-value2'&lt;/pre&gt; &lt;p&gt;It includes six configuration options, which you can fill out in the GUI:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;In &lt;em&gt;Tasks&lt;/em&gt;, select &lt;em&gt;Update ConfigMap&lt;/em&gt; to access the dialog shown in Figure 7:&lt;/li&gt; &lt;/ol&gt; &lt;div id="attachment_651137" style="width: 510px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-651137" class="wp-image-651137" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/config-map-task-300x249.png" alt="" width="500" height="414" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/config-map-task-300x249.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/config-map-task.png 683w" sizes="(max-width: 500px) 100vw, 500px" /&gt;&lt;p id="caption-attachment-651137" class="wp-caption-text"&gt;Figure 7: Updating a ConfigMap.&lt;/p&gt;&lt;/div&gt; &lt;ol start="2"&gt; &lt;li&gt;In the &lt;em&gt;OpenShift service connection&lt;/em&gt; drop-down box, select the service connection you just created, which will be used to execute this command.&lt;/li&gt; &lt;li&gt;In the &lt;em&gt;Version of oc&lt;/em&gt; text box, add the version of &lt;code&gt;oc&lt;/code&gt; you want to use (e.g., 3.11.154) or a direct URL to an &lt;code&gt;oc&lt;/code&gt; release bundle. (If left blank, the latest stable oc version is used.)&lt;/li&gt; &lt;li&gt;In the &lt;em&gt;Name of the ConfigMap&lt;/em&gt; text box, enter the name of the &lt;code&gt;ConfigMap&lt;/code&gt; to update. (This field is required.)&lt;/li&gt; &lt;li&gt;In the &lt;em&gt;Namespace of ConfigMap&lt;/em&gt; text box, enter the namespace in which to find the &lt;code&gt;ConfigMap&lt;/code&gt;. The current namespace is used if none is specified.&lt;/li&gt; &lt;li&gt;In the &lt;em&gt;ConfigMap Properties&lt;/em&gt; text box, enter the properties to set or update. Only the properties which need creating or updating need to be listed. Space-separated values need to be surrounded by quotes (&amp;#8220;).&lt;/li&gt; &lt;li&gt;Check or un-check the &lt;em&gt;use local oc executable&lt;/em&gt; checkbox, which specified whether to force the extension to use, if present, the &lt;code&gt;oc&lt;/code&gt; CLI found on the machine containing the agent. &lt;em&gt;If no version is specified&lt;/em&gt;, the extension uses the local &lt;code&gt;oc&lt;/code&gt; CLI no matter what its version is. &lt;em&gt;If a version is specified&lt;/em&gt;, then the extension checks to see if the &lt;code&gt;oc&lt;/code&gt; CLI installed has the same version requested by the user (if not, the correct &lt;code&gt;oc&lt;/code&gt; CLI will be downloaded).&lt;/li&gt; &lt;/ol&gt; &lt;h3&gt;Work with OpenShift&lt;/h3&gt; &lt;p&gt;It is finally time to create your YAML pipeline by using the OpenShift VSTS extension. In our example, we have the application &lt;code&gt;nodejs-ex&lt;/code&gt; already running on our OpenShift cluster, and our goal is to create a pipeline to push a new version of our application whenever our GitHub master branch is updated. Here is our task:&lt;/p&gt; &lt;pre&gt;jobs: - job: demo   displayName: MyDemo   pool:     name: 'Default'   steps:   - task: oc-cmd@2     inputs:       openshiftService: 'My Openshift'       cmd: 'oc start-build nodejs-ex --follow'       uselocalOc: true   - task: oc-cmd@2     inputs:       openshiftService: 'My Openshift'       cmd: 'oc status'       uselocalOc: true&lt;/pre&gt; &lt;p&gt;Every time the pipeline is triggered, a new build starts, and our application is pushed to the cluster eventually. It is important to note that because we are using a local agent to run this pipeline (which is on a machine with the &lt;code&gt;oc&lt;/code&gt; CLI already installed, we set the flag &lt;code&gt;uselocalOc&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt; and did not specify any version. The extension will use the &lt;code&gt;oc&lt;/code&gt; CLI that is installed on the machine, whatever its version is.&lt;/p&gt; &lt;p&gt;Next, we check the status of our cluster to see if there are any misconfigured components (services, deployment configs, build configurations, or active deployments).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you want to use a specific &lt;code&gt;oc&lt;/code&gt; version, be sure to type it correctly, otherwise the latest release will be used (e.g., if you type &lt;code&gt;v3.5&lt;/code&gt; as your version input, the extension will download version 3.5.5, because 3.5 does not exist in our repo. Check the &lt;a href="https://github.com/redhat-developer/openshift-vsts/blob/master/README.md" target="_blank" rel="noopener noreferrer"&gt;README&lt;/a&gt; file for more information).&lt;/p&gt; &lt;h2&gt;Wrapping up&lt;/h2&gt; &lt;p&gt;At this point, you should be able to set up your OpenShift VSTS extension and use it to create your own YAML-defined pipeline, then deploy your application to your OpenShift cluster from Azure DevOps. OpenShift VSTS is an open source project, and we welcome contributions and suggestions. Please reach out to us if you have any requests for further deployments, ideas to improve the extension, questions, or if you encounter any issues. Contacting us is simple: &lt;a href="https://github.com/redhat-developer/openshift-vsts" target="_blank" rel="noopener noreferrer"&gt;Open a new issue&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F05%2Fintroduction-to-the-red-hat-openshift-deployment-extension-for-microsoft-azure-devops%2F&amp;#38;linkname=Introduction%20to%20the%20Red%20Hat%20OpenShift%20deployment%20extension%20for%20Microsoft%20Azure%20DevOps" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F05%2Fintroduction-to-the-red-hat-openshift-deployment-extension-for-microsoft-azure-devops%2F&amp;#38;linkname=Introduction%20to%20the%20Red%20Hat%20OpenShift%20deployment%20extension%20for%20Microsoft%20Azure%20DevOps" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F05%2Fintroduction-to-the-red-hat-openshift-deployment-extension-for-microsoft-azure-devops%2F&amp;#38;linkname=Introduction%20to%20the%20Red%20Hat%20OpenShift%20deployment%20extension%20for%20Microsoft%20Azure%20DevOps" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F05%2Fintroduction-to-the-red-hat-openshift-deployment-extension-for-microsoft-azure-devops%2F&amp;#38;linkname=Introduction%20to%20the%20Red%20Hat%20OpenShift%20deployment%20extension%20for%20Microsoft%20Azure%20DevOps" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F05%2Fintroduction-to-the-red-hat-openshift-deployment-extension-for-microsoft-azure-devops%2F&amp;#38;linkname=Introduction%20to%20the%20Red%20Hat%20OpenShift%20deployment%20extension%20for%20Microsoft%20Azure%20DevOps" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F05%2Fintroduction-to-the-red-hat-openshift-deployment-extension-for-microsoft-azure-devops%2F&amp;#38;linkname=Introduction%20to%20the%20Red%20Hat%20OpenShift%20deployment%20extension%20for%20Microsoft%20Azure%20DevOps" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F05%2Fintroduction-to-the-red-hat-openshift-deployment-extension-for-microsoft-azure-devops%2F&amp;#38;linkname=Introduction%20to%20the%20Red%20Hat%20OpenShift%20deployment%20extension%20for%20Microsoft%20Azure%20DevOps" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F05%2Fintroduction-to-the-red-hat-openshift-deployment-extension-for-microsoft-azure-devops%2F&amp;#038;title=Introduction%20to%20the%20Red%20Hat%20OpenShift%20deployment%20extension%20for%20Microsoft%20Azure%20DevOps" data-a2a-url="https://developers.redhat.com/blog/2019/12/05/introduction-to-the-red-hat-openshift-deployment-extension-for-microsoft-azure-devops/" data-a2a-title="Introduction to the Red Hat OpenShift deployment extension for Microsoft Azure DevOps"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/12/05/introduction-to-the-red-hat-openshift-deployment-extension-for-microsoft-azure-devops/"&gt;Introduction to the Red Hat OpenShift deployment extension for Microsoft Azure DevOps&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/KOJekK8uNAg" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;We are extremely pleased to present the new version of the Red Hat OpenShift deployment extension (OpenShift VSTS) 1.4.0 for Microsoft Azure DevOps. This extension enables users to deploy their applications to any OpenShift cluster directly from their Microsoft Azure DevOps account. In this article, we will look at how to install and use this [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/12/05/introduction-to-the-red-hat-openshift-deployment-extension-for-microsoft-azure-devops/"&gt;Introduction to the Red Hat OpenShift deployment extension for Microsoft Azure DevOps&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><post-id xmlns="com-wordpress:feed-additions:1">650877</post-id><dc:creator>Luca Stocchi</dc:creator><dc:date>2019-12-05T08:00:11Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/12/05/introduction-to-the-red-hat-openshift-deployment-extension-for-microsoft-azure-devops/</feedburner:origLink></entry><entry><title>Understanding Red Hat AMQ Streams components for OpenShift and Kubernetes: Part 2</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/XV4puC8LLGM/" /><category term="Event-Driven" /><category term="Red Hat AMQ" /><category term="Red Hat OpenShift Container Platform" /><category term="Apache Kafka" /><category term="Red Hat AMQ Streams" /><category term="Strimzi" /><author><name>Pramod Padmanabhan</name></author><id>https://developers.redhat.com/blog/?p=652087</id><updated>2019-12-05T08:00:07Z</updated><published>2019-12-05T08:00:07Z</published><content type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/blog/2019/11/20/red-hat-amq-stre…ubernetes-part-1/"&gt;In the previous article in this series&lt;/a&gt;, we discussed the basics of &lt;a href="https://access.redhat.com/products/red-hat-amq" target="_blank" rel="noopener noreferrer"&gt;Red Hat AMQ Streams&lt;/a&gt; on &lt;a href="http://developers.redhat.com/openshift/"&gt;Red Hat OpenShift&lt;/a&gt;. Here are a few key points to keep in mind before we proceed:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AMQ Streams is based on Apache Kafka.&lt;/li&gt; &lt;li&gt;AMQ Streams for the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.5/html/using_amq_streams_on_openshift_container_platform/"&gt;OpenShift Container Platform&lt;/a&gt; is based on the Strimzi project.&lt;/li&gt; &lt;li&gt;AMQ Streams on containers has multiple components, such as the Cluster Operator, Entity Operator, Mirror Maker, Kafka connect, and Kafka Bridge.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Now, let&amp;#8217;s continue on to setting up Kafka Connect, the Kafka Bridge, and Mirror Maker.&lt;/p&gt; &lt;h2&gt;Kafka Connect&lt;/h2&gt; &lt;p&gt;Kafka Connect is mainly used to stream data &lt;em&gt;in&lt;/em&gt; and &lt;em&gt;out&lt;/em&gt; of Kafka clusters; for instance, getting a Twitter feed and then pushing it to the cluster. We need to understand Kafka Connect&amp;#8217;s concepts before continuing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Connectors define where the data should be copied &lt;em&gt;to&lt;/em&gt; or &lt;em&gt;from&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;Tasks are the actors that actually copy the data.&lt;/li&gt; &lt;li&gt;Workers are used to schedule units of work for &lt;em&gt;connectors&lt;/em&gt; and &lt;em&gt;tasks&lt;/em&gt;.&lt;/li&gt; &lt;li&gt;Converters are used by &lt;em&gt;task units&lt;/em&gt; to change data format.&lt;/li&gt; &lt;li&gt;Transforms are used by &lt;em&gt;connector&lt;/em&gt; units to do simple data adjustments, routing, and chain transformations.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; For more details on the basic concepts, I recommend reading &lt;a href="https://docs.confluent.io/current/connect/concepts.html#connect-concepts" target="_blank" rel="noopener noreferrer"&gt;Kafka Connect Concepts&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Creating a simple Kafka Connect instance&lt;/h3&gt; &lt;p&gt;A Kafka Connect instance in OpenShift can be created using two different Kube objects: KafkaConnect and KafkaConnectS2I. By default, Kafka Connect includes two built-in connectors: FileStreamSourceConnector and FileStreamSinkConnector. However, before you build a new connector, first check the &lt;a href="https://www.confluent.io/hub/" target="_blank" rel="noopener noreferrer"&gt;catalog of existing connectors&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Let us set up a simple Kafka Connect instance and then perform the source and sink operations. Then, we can add a default producer sample app and a consumer sample app. This process will show multiple publishers and multiple consumers, as shown in Figure 1:&lt;/p&gt; &lt;div id="attachment_652237" style="width: 510px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca1fbebd0d0.png"&gt;&lt;img aria-describedby="caption-attachment-652237" class="wp-image-652237" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca1fbebd0d0-300x214.png" alt="The structure for our example." width="500" height="357" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca1fbebd0d0-300x214.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca1fbebd0d0-768x548.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca1fbebd0d0.png 943w" sizes="(max-width: 500px) 100vw, 500px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-652237" class="wp-caption-text"&gt;Figure 1: The overall structure for this example.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Begin by creating the Kafka Connect config &lt;code&gt;amq-kafka-connect.yml&lt;/code&gt;. The example file present in &lt;code&gt;examples/kafka-connect/kafka-connect.yml&lt;/code&gt; was used as a reference for this config file:&lt;/p&gt; &lt;pre&gt;apiVersion: kafka.strimzi.io/v1beta1 kind: KafkaConnect metadata: name: simple-connect-cluster spec: version: 2.3.0 replicas: 1 bootstrapServers: simple-cluster-kafka-bootstrap:9093 tls: trustedCertificates: - secretName: simple-cluster-cluster-ca-cert certificate: ca.crt&lt;/pre&gt; &lt;p&gt;Next, execute the YAML:&lt;/p&gt; &lt;pre&gt;$ oc create -f amq-kafka-connect.yml&lt;/pre&gt; &lt;p&gt;You can see the result in Figure 2:&lt;/p&gt; &lt;div id="attachment_652207" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dc9fc4d92849.png"&gt;&lt;img aria-describedby="caption-attachment-652207" class="wp-image-652207 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dc9fc4d92849-1024x115.png" alt="Kafka Connect is is deployed." width="640" height="72" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dc9fc4d92849-1024x115.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dc9fc4d92849-300x34.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dc9fc4d92849-768x87.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dc9fc4d92849.png 1348w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-652207" class="wp-caption-text"&gt;Figure 2: The new deployment is in place.&lt;/p&gt;&lt;/div&gt; &lt;h3&gt;Preparing to test your new Kafka Connect instance&lt;/h3&gt; &lt;p&gt;Let us set up to test the new instance by doing the following:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Create a new config, which will &lt;em&gt;sink&lt;/em&gt; from the &lt;code&gt;redhat-demo-topics&lt;/code&gt; topic content to the file &lt;code&gt;amq-demo-sink.txt&lt;/code&gt;:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;$ oc rsh simple-cluster-kafka-0 sh-4.2$ curl -X POST -H "Content-Type: application/json" --data '{"name": "redhat-file-sink-demo", "config": {"connector.class":"FileStreamSinkConnector", "tasks.max":"1", "file":"/tmp/amq-demo-sink.txt", "topic":"redhat-demo-topics", "value.converter.schemas.enable" : "false", "value.converter" : "org.apache.kafka.connect.storage.StringConverter", "value.converter.schemas.enable" : "false", "key.converter" : "org.apache.kafka.connect.storage.StringConverter", "key.converter.schemas.enable" : "false"}}' http://simple-connect-cluster-connect-api.amq-streams.svc:8083/connectors&lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;Here is the output:&lt;/p&gt; &lt;pre&gt;{"name":"redhat-file-sink-demo","config":{"connector.class":"FileStreamSinkConnector","tasks.max":"1","file":"/tmp/amq-demo-sink.txt","topics":"redhat-demo-topics","value.converter.schemas.enable":"false","value.converter":"org.apache.kafka.connect.storage.StringConverter","key.converter":"org.apache.kafka.connect.storage.StringConverter","key.converter.schemas.enable":"false","name":"redhat-file-sink-demo"},"tasks":[],"type":"sink"}&lt;/pre&gt; &lt;ol start="2"&gt; &lt;li&gt;Create a new config that will &lt;em&gt;source&lt;/em&gt; into the &lt;code&gt;redhat-demo-topics&lt;/code&gt; topic content from the file &lt;code&gt;amq-demo-source.txt&lt;/code&gt;:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;$ oc rsh simple-cluster-kafka-0 sh-4.2$ curl -X POST -H "Content-Type: application/json" --data '{"name": "redhat-file-source-demo", "config": {"connector.class":"FileStreamSourceConnector", "tasks.max":"1", "file":"/tmp/amq-demo-source.txt", "topic":"redhat-demo-topics", "value.converter.schemas.enable" : "false", "value.converter" : "org.apache.kafka.connect.storage.StringConverter", "value.converter.schemas.enable" : "false", "key.converter" : "org.apache.kafka.connect.storage.StringConverter", "key.converter.schemas.enable" : "false"}}' http://simple-connect-cluster-connect-api.amq-streams.svc:8083/connectors&lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;Here is the output:&lt;/p&gt; &lt;pre&gt;{"name":"redhat-file-source-demo","config":{"connector.class":"FileStreamSourceConnector","tasks.max":"1","file":"/tmp/amq-demo-source.txt","topic":"redhat-demo-topics","value.converter.schemas.enable":"false","value.converter":"org.apache.kafka.connect.storage.StringConverter","key.converter":"org.apache.kafka.connect.storage.StringConverter","key.converter.schemas.enable":"false","name":"redhat-file-source-demo"},"tasks":[],"type":"source"}&lt;/pre&gt; &lt;ol start="3"&gt; &lt;li&gt;In a new terminal start the producer sample app:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;$ oc run kafka-producer -ti --image=registry.redhat.io/amq7/amq-streams-kafka-23:1.3.0 --rm=true --restart=Never -- bin/kafka-console-producer.sh --broker-list simple-cluster-kafka-bootstrap:9092 --topic redhat-demo-topics&lt;/pre&gt; &lt;ol start="4"&gt; &lt;li&gt;In a new terminal start the consumer sample app:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;$ oc run kafka-consumer -ti --image=registry.redhat.io/amq7/amq-streams-kafka-23:1.3.0 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server simple-cluster-kafka-bootstrap:9092 --topic redhat-demo-topics --from-beginning&lt;/pre&gt; &lt;h3&gt;Testing your new Kafka Connect instance&lt;/h3&gt; &lt;p&gt;To test your instance, do the following:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Log into the Kafka Connect pod and watch the &lt;code&gt;/tmp/amq-demo-sink.txt&lt;/code&gt; file:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;$ oc get po | grep connect simple-connect-cluster-connect-7479b86c7-tmdbp 1/1 Running 0 3h $ oc rsh simple-connect-cluster-connect-7479b86c7-tmdbp sh-4.2$ tail -100f /tmp/amq-demo-sink.txt hello world from pramod&lt;/pre&gt; &lt;p&gt;Here, you can see that the connector has already sunk two messages into the file.&lt;/p&gt; &lt;ol start="2"&gt; &lt;li&gt; Log into the Kafka Connect pod in a different terminal, then add content into &lt;code&gt;/tmp/amq-demo-source.txt&lt;/code&gt;:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;$ oc rsh simple-connect-cluster-connect-7479b86c7-tmdbp sh-4.2$ echo redhat-is-my-world &amp;#62; /tmp/amq-demo-source.txt&lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;This set of commands writes a message in &lt;code&gt;/tmp/amq-demo-sink.txt&lt;/code&gt;, and also to the consumer sample app. Figure 3 shows the connector source push and sink output:&lt;/p&gt; &lt;div id="attachment_652277" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca293c7946a.png"&gt;&lt;img aria-describedby="caption-attachment-652277" class="wp-image-652277 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca293c7946a-1024x270.png" alt="The connector's push and sink output." width="640" height="169" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca293c7946a-1024x270.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca293c7946a-300x79.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca293c7946a-768x203.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca293c7946a.png 1380w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-652277" class="wp-caption-text"&gt;Figure 3: The connector source pushing the message redhat-is-my-world in the second terminal.&lt;/p&gt;&lt;/div&gt; &lt;p style="padding-left: 40px;"&gt;Figure 4 shows the sample app consuming the message:&lt;/p&gt; &lt;div id="attachment_652297" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca2a45c034c.png"&gt;&lt;img aria-describedby="caption-attachment-652297" class="wp-image-652297 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca2a45c034c-1024x83.png" alt="The sample app consuming the message." width="640" height="52" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca2a45c034c-1024x83.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca2a45c034c-300x24.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca2a45c034c-768x62.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-652297" class="wp-caption-text"&gt;Figure 4: The results on the consuming side.&lt;/p&gt;&lt;/div&gt; &lt;ol start="3"&gt; &lt;li&gt;Now, send a message from the producer sample app. Figure 5 shows how this message flows through the system in three parts. From top to bottom, these are the connector sink, the consumer sample app, and then the producer sample app pushing the message &lt;code&gt;kafka connect sample app&lt;/code&gt;:&lt;/li&gt; &lt;/ol&gt; &lt;div id="attachment_652317" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca2be40e1d3.png"&gt;&lt;img aria-describedby="caption-attachment-652317" class="wp-image-652317 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca2be40e1d3-1024x444.png" alt="The message flowing through each step." width="640" height="278" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca2be40e1d3-1024x444.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca2be40e1d3-300x130.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dca2be40e1d3-768x333.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-652317" class="wp-caption-text"&gt;Figure 5: The message kafka connect sample app passing through each stage.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Kafka Bridge&lt;/h2&gt; &lt;p&gt;The Bridge component helps us connect to the Kafka Cluster using the HTTP or AMQP protocol. In this article, we demo the HTTP usage as shown in Figure 6:&lt;/p&gt; &lt;div id="attachment_652947" style="width: 510px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcb330887257.png"&gt;&lt;img aria-describedby="caption-attachment-652947" class="wp-image-652947" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcb330887257-300x215.png" alt="Our structure including the Kafka Bridge." width="500" height="358" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcb330887257-300x215.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcb330887257-768x549.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcb330887257.png 938w" sizes="(max-width: 500px) 100vw, 500px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-652947" class="wp-caption-text"&gt;Figure 6: How the Kafka Bridge fits in through the HTTP protocol.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Kafka Bridge produces a REST API for the HTTP protocol, through which it provides multiple operations, such as:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sending messages.&lt;/li&gt; &lt;li&gt;Subscribing to topics.&lt;/li&gt; &lt;li&gt;Receiving messages.&lt;/li&gt; &lt;li&gt;Committing offsets.&lt;/li&gt; &lt;li&gt;Seeking specific positions.&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Creating your Kafka Bridge&lt;/h3&gt; &lt;ol&gt; &lt;li&gt;Create the &lt;code&gt;kafka-bridge&lt;/code&gt; config file &lt;code&gt;amq-kafka-bridge.yml&lt;/code&gt;. The example file present in &lt;code&gt;examples/kafka-bridge/kafka-bridge.yaml&lt;/code&gt; was used as a reference for the following config:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;apiVersion: kafka.strimzi.io/v1alpha1 kind: KafkaBridge metadata: name: simple-bridge spec: replicas: 1 bootstrapServers: simple-cluster-kafka-bootstrap:9092 http: port: 8080&lt;/pre&gt; &lt;ol start="2"&gt; &lt;li&gt;Create the bridge in OCP:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;$ oc create -f amq-kafka-bridge.yml&lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;You can see the results in Figure 7:&lt;/p&gt; &lt;div id="attachment_652977" style="width: 510px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcb36ae26e05.png"&gt;&lt;img aria-describedby="caption-attachment-652977" class="wp-image-652977" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcb36ae26e05-300x38.png" alt="The Kafka Bridge is deployed." width="500" height="63" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcb36ae26e05-300x38.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcb36ae26e05.png 726w" sizes="(max-width: 500px) 100vw, 500px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-652977" class="wp-caption-text"&gt;Figure 7: Your new Kafka Bridge.&lt;/p&gt;&lt;/div&gt; &lt;ol start="3"&gt; &lt;li&gt;Create a route so that we can access the bridge from outside the cluster:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;$ oc expose svc simple-bridge-bridge-service --name=simple-bridge-route&lt;/pre&gt; &lt;h3&gt;Testing your HTTP protocol-based Kafka Bridge&lt;/h3&gt; &lt;ol&gt; &lt;li&gt;Create the Kafka topic config &lt;code&gt;amq-kafka-topic.yml&lt;/code&gt; and apply it to the cluster:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;apiVersion: kafka.strimzi.io/v1beta1 kind: KafkaTopic metadata: name: simple-topic labels: strimzi.io/cluster: simple-cluster spec: partitions: 5 replicas: 1 config: retention.ms: 7200000 segment.bytes: 1073741824 oc create -f amq-kafka-topic.yml&lt;/pre&gt; &lt;ol start="2"&gt; &lt;li&gt;Get the route URL for the bridge endpoints:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;#get the route to do the curl command oc get route NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD simple-bridge-route simple-bridge-route-amq-streams.apps.redhat.demo.com simple-bridge-bridge-service rest-api None&lt;/pre&gt; &lt;ol start="3"&gt; &lt;li&gt;Publish a message on &lt;code&gt;simple-topic&lt;/code&gt;:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;curl -X POST \ http://simple-bridge-route-amq-streams.apps.redhat.demo.com/topics/simple-topic \ -H 'content-type: application/vnd.kafka.json.v2+json' \ -d '{ "records": [ { "value": "all hail the shadowman" } ] }'&lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;Here is the output:&lt;/p&gt; &lt;pre&gt;{"offsets":[{"partition":0,"offset":0}]}&lt;/pre&gt; &lt;ol start="4"&gt; &lt;li&gt;Create the consumer group &lt;code&gt;simple-rh-bridge-consumer-group&lt;/code&gt; and the instance &lt;code&gt;simple-rh-bridge-consumer&lt;/code&gt;. For this task, we set the message format to JSON:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;curl -X POST \ http://simple-bridge-route-amq-streams.apps.redhat.demo.com/consumers/simple-rh-bridge-consumer-group \ -H 'content-type: application/vnd.kafka.v2+json' \ -d '{ "name": "simple-rh-bridge-consumer", "auto.offset.reset": "earliest", "format": "json", "enable.auto.commit": false, "fetch.min.bytes": 512, "consumer.request.timeout.ms": 30000 }'&lt;/pre&gt; &lt;ol start="5"&gt; &lt;li&gt;Create a subscriber for the &lt;code&gt;simple-topic&lt;/code&gt; created in step one:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;curl -X POST http://simple-bridge-route-amq-streams.apps.redhat.demo.com/consumers/simple-rh-bridge-consumer-group/instances/simple-rh-bridge-consumer/subscription \ -H 'content-type: application/vnd.kafka.v2+json' \ -d '{ "topics": [ "simple-topic" ] }'&lt;/pre&gt; &lt;ol start="6"&gt; &lt;li&gt;Consume the messages (note that the first request will register and the subsequent calls will provide the array of messages):&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;curl -X GET http://simple-bridge-route-amq-streams.apps.redhat.demo.com/consumers/simple-rh-bridge-consumer-group/instances/simple-rh-bridge-consumer/records \ -H 'accept: application/vnd.kafka.json.v2+json'&lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;Here is the output:&lt;/p&gt; &lt;pre&gt;[] curl -X GET http://simple-bridge-route-amq-streams.apps.redhat.demo.com/consumers/simple-rh-bridge-consumer-group/instances/simple-rh-bridge-consumer/records \ -H 'accept: application/vnd.kafka.json.v2+json'&lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;And the additional output:&lt;/p&gt; &lt;pre&gt;[{"topic":"simple-topic","key":null,"value":"all hail the shadowman","partition":0,"offset":0}]&lt;/pre&gt; &lt;h2&gt;Mirror Maker&lt;/h2&gt; &lt;p&gt;Kafka Mirror Maker replicates data from one Kafka cluster to another. The usual use case is across different data centers.&lt;/p&gt; &lt;p&gt;For the purpose of this demo, we use two different namespaces and projects, namely &lt;code&gt;amq-streams&lt;/code&gt; and &lt;code&gt;amq-streams-dc2&lt;/code&gt;. Doing so is the same as having multiple data centers with the same names. This setup is shown in Figure 8:&lt;/p&gt; &lt;div id="attachment_653107" style="width: 510px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcb7b940409f.png"&gt;&lt;img aria-describedby="caption-attachment-653107" class="wp-image-653107" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcb7b940409f-300x212.png" alt="Our structure for replicating multiple data centers." width="500" height="354" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcb7b940409f-300x212.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcb7b940409f-768x543.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcb7b940409f.png 942w" sizes="(max-width: 500px) 100vw, 500px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-653107" class="wp-caption-text"&gt;Figure 8: Replicating multiple data centers with Kafka Mirror Maker.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Kafka Mirror maker consumes from the active Kafka cluster and produces to the mirror (backup) Kafka cluster.&lt;/p&gt; &lt;h3&gt;Setting up for the example&lt;/h3&gt; &lt;p&gt;To demo the Kafka Mirror Maker, we need to create another namespace and Kafka cluster. First, create the new namespace &lt;code&gt;amq-streams-dc2&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;$ oc new-project amq-streams-dc2&lt;/pre&gt; &lt;p&gt;Next, create a new Kafka cluster:&lt;/p&gt; &lt;pre&gt;$ sed -i 's/namespace: .*/namespace: amq-streams-dc2/' install/cluster-operator/*RoleBinding*.yaml&lt;/pre&gt; &lt;p&gt;On macOS, use the following instead:&lt;/p&gt; &lt;pre&gt;$ sed -i '' 's/namespace: .*/namespace: amq-streams-dc2/' install/cluster-operator/*RoleBinding*.yaml $ oc apply -f install/cluster-operator -n amq-streams-dc2 $ oc apply -f amq-kafka-cluster.yml -n amq-streams-dc2&lt;/pre&gt; &lt;p&gt;You can see the result in Figure 9:&lt;/p&gt; &lt;div id="attachment_653477" style="width: 510px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcc8dd449192.png"&gt;&lt;img aria-describedby="caption-attachment-653477" class="wp-image-653477" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcc8dd449192-300x178.png" alt="All of the components installed so far." width="500" height="296" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcc8dd449192-300x178.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcc8dd449192-768x455.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcc8dd449192.png 941w" sizes="(max-width: 500px) 100vw, 500px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-653477" class="wp-caption-text"&gt;Figure 9: All of the pieces are in place so far.&lt;/p&gt;&lt;/div&gt; &lt;h3&gt;Creating a mirror with Kafka Mirror Maker&lt;/h3&gt; &lt;p&gt;Create a &lt;code&gt;kafka-mirror-maker&lt;/code&gt; config &lt;code&gt;amq-kafka-mirror-maker.yml&lt;/code&gt;. In this file, we increase the consumer stream to two for faster response and use the group ID &lt;code&gt;simple-source-group-id&lt;/code&gt; for the consumer. Additionally, we whitelist all of the topics using wildcards. This example file present in &lt;code&gt;examples/kafka-mirror-maker/kafka-mirror-maker.yaml&lt;/code&gt; was used as a reference for the config:&lt;/p&gt; &lt;div&gt; &lt;div&gt; &lt;pre&gt;apiVersion: kafka.strimzi.io/v1beta1 kind: KafkaMirrorMaker metadata: name: simple-mirror-maker spec: version: 2.3.0 replicas: 1 consumer: bootstrapServers: simple-cluster-kafka-bootstrap:9092 groupId: simple-source-group-id numStreams: 2 producer: bootstrapServers: simple-cluster-kafka-bootstrap.amq-streams-dc2.svc:9092 whitelist: ".*"&lt;/pre&gt; &lt;div&gt;Create the Mirror Maker in the &lt;code&gt;amq-streams&lt;/code&gt; namespace:&lt;/div&gt; &lt;/div&gt; &lt;pre&gt;$ oc create -f amq-kafka-mirror-maker.yml -n amq-streams&lt;/pre&gt; &lt;div&gt; &lt;p&gt;You can see the result in Figure 10:&lt;/p&gt; &lt;div id="attachment_653487" style="width: 510px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcc8df7977ec.png"&gt;&lt;img aria-describedby="caption-attachment-653487" class="wp-image-653487" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcc8df7977ec-300x30.png" alt="Mirror Maker is deployed." width="500" height="50" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcc8df7977ec-300x30.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcc8df7977ec.png 729w" sizes="(max-width: 500px) 100vw, 500px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-653487" class="wp-caption-text"&gt;Figure 10: Your new Mirror Maker instance.&lt;/p&gt;&lt;/div&gt; &lt;/div&gt; &lt;h3&gt;Testing Kafka Mirror Maker&lt;/h3&gt; &lt;p&gt;To test the Mirror Maker, create the following two namespaces: &lt;code&gt;amq-streams&lt;/code&gt; and &lt;code&gt;amq-streams-dc2&lt;/code&gt;. The &lt;code&gt;amq-streams&lt;/code&gt; namespace will contain the producer sample app to produce new messages, and the consumer sample app to consume new messages. The &lt;code&gt;amq-streams-dc2&lt;/code&gt; namespace will contain the consumer sample app so it can consume new messages, so it can show that the messages are getting pushed to the DC2 cluster.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Create a producer sample app and consumer sample app in the &lt;code&gt;amq-streams&lt;/code&gt; namespace:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;$ oc project amq-streams $ oc run kafka-producer -ti --image=registry.redhat.io/amq7/amq-streams-kafka-23:1.3.0 --rm=true --restart=Never -- bin/kafka-console-producer.sh --broker-list simple-cluster-kafka-bootstrap:9092 --topic redhat-demo-topics $ oc run kafka-consumer -ti --image=registry.redhat.io/amq7/amq-streams-kafka-23:1.3.0 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server simple-cluster-kafka-bootstrap:9092 --topic redhat-demo-topics&lt;/pre&gt; &lt;ol start="2"&gt; &lt;li&gt;Create a Consumer sample app in the &lt;code&gt;amq-streams-dc2&lt;/code&gt; namespace&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;$ oc project amq-streams-dc2 $ oc run kafka-consumer -ti --image=registry.redhat.io/amq7/amq-streams-kafka-23:1.3.0 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server simple-cluster-kafka-bootstrap:9092 --topic redhat-demo-topics&lt;/pre&gt; &lt;ol start="3"&gt; &lt;li&gt;Send a message from the producer sample app.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Is Mirror Maker working? You should see the message in the consumer app in both the namespaces, as shown in Figure 11:&lt;/p&gt; &lt;div id="attachment_653507" style="width: 510px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcc92af84135.png"&gt;&lt;img aria-describedby="caption-attachment-653507" class="wp-image-653507" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcc92af84135-300x144.png" alt="The message flowing through the components." width="500" height="239" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcc92af84135-300x144.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcc92af84135-768x368.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcc92af84135.png 942w" sizes="(max-width: 500px) 100vw, 500px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-653507" class="wp-caption-text"&gt;Figure 11: Your message flowing from the producer to both the consumer and the backup consumer.&lt;/p&gt;&lt;/div&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this article, we explored Red Hat AMQ Streams components like Kafka Connect, Kafka Bridge, and Mirror Maker. In the third and final part of the series, we will cover monitoring and administration.&lt;/p&gt; &lt;h3&gt;References&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://docs.confluent.io/current/connect/index.html" target="_blank" rel="noopener noreferrer"&gt;Kafka Connect&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.5/html-single/using_amq_streams_on_openshift/index" target="_blank" rel="noopener noreferrer"&gt;Using AMQ Streams on OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/ppatierno/amqp-kafka-demo" target="_blank" rel="noopener noreferrer"&gt;AMQP Kafka Demo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://strimzi.io/docs/latest/" target="_blank" rel="noopener noreferrer"&gt;Using Strimzi (latest)&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F05%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-2%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%202" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F05%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-2%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%202" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F05%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-2%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%202" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F05%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-2%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%202" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F05%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-2%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%202" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F05%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-2%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%202" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F05%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-2%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%202" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F05%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-2%2F&amp;#038;title=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%202" data-a2a-url="https://developers.redhat.com/blog/2019/12/05/understanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-2/" data-a2a-title="Understanding Red Hat AMQ Streams components for OpenShift and Kubernetes: Part 2"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/12/05/understanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-2/"&gt;Understanding Red Hat AMQ Streams components for OpenShift and Kubernetes: Part 2&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/XV4puC8LLGM" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;In the previous article in this series, we discussed the basics of Red Hat AMQ Streams on Red Hat OpenShift. Here are a few key points to keep in mind before we proceed: AMQ Streams is based on Apache Kafka. AMQ Streams for the OpenShift Container Platform is based on the Strimzi project. AMQ Streams [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/12/05/understanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-2/"&gt;Understanding Red Hat AMQ Streams components for OpenShift and Kubernetes: Part 2&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><post-id xmlns="com-wordpress:feed-additions:1">652087</post-id><dc:creator>Pramod Padmanabhan</dc:creator><dc:date>2019-12-05T08:00:07Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/12/05/understanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-2/</feedburner:origLink></entry><entry><title>Narayana 5.10.1.Final released</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/-t0Osugw5bA/narayana-5101final-released.html" /><category term="feed_group_name_jbosstransactions" scheme="searchisko:content:tags" /><category term="feed_name_transactions" scheme="searchisko:content:tags" /><category term="narayana transaction manager release 5.10.1.Final" scheme="searchisko:content:tags" /><author><name>Michael Musgrove</name></author><id>searchisko:content:id:jbossorg_blog-narayana_5_10_1_final_released</id><updated>2019-12-04T15:17:33Z</updated><published>2019-12-04T15:17:00Z</published><content type="html">The team are pleased to announce our latest release of Narayana - the premier open source transaction manager.&lt;br /&gt;&lt;br /&gt;The release is available for &lt;a href="http://narayana.io/downloads/index.html"&gt;download&lt;/a&gt; from our website.&lt;br /&gt;It is a bug fix release and a list of what we fixed is available in the &lt;a href="https://issues.jboss.org/secure/ReleaseNote.jspa?projectId=12310200&amp;amp;version=12343104"&gt;release notes&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;We always generate comparative TPS figures for each release and this new release &lt;a href="https://github.com/jbosstm/artifacts/blob/master/jobs/tm-comparison/benchmark.png"&gt;performs&lt;/a&gt; favourably against other open source transaction managers.&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/-t0Osugw5bA" height="1" width="1" alt=""/&gt;</content><summary>The team are pleased to announce our latest release of Narayana - the premier open source transaction manager. The release is available for download from our website. It is a bug fix release and a list of what we fixed is available in the release notes. We always generate comparative TPS figures for each release and this new release performs favourably against other open source transaction manager...</summary><dc:creator>Michael Musgrove</dc:creator><dc:date>2019-12-04T15:17:00Z</dc:date><feedburner:origLink>http://jbossts.blogspot.com/2019/12/narayana-5101final-released.html</feedburner:origLink></entry><entry><title>Apache Camel 3 - Whats New Top 10</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Fa5Om9oz54s/apache-camel-3-whats-new-top-10.html" /><category term="apache camel" scheme="searchisko:content:tags" /><category term="feed_group_name_fusesource" scheme="searchisko:content:tags" /><category term="feed_name_clausibsen" scheme="searchisko:content:tags" /><category term="release" scheme="searchisko:content:tags" /><category term="roadmap" scheme="searchisko:content:tags" /><author><name>Claus Ibsen</name></author><id>searchisko:content:id:jbossorg_blog-apache_camel_3_whats_new_top_10</id><updated>2019-12-04T11:02:14Z</updated><published>2019-12-04T11:01:00Z</published><content type="html">&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;Apache Camel 3 was released last thursday November 28th 2019, which also happens to be the day of the US Thanksgiving. This was not intentionally but we can say its a big thanks from us to the community with a brand new major version of Camel - this does not come by often. In fact it's 10 years since Camel 2 hit the streets. So this 3rd generation is long overdue.&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://camel.apache.org/blog/Camel3-Whatsnew/camel3-3humps_hu7ba293cf525bac57712ef8f5199c56b4_332440_800x0_resize_q95_gaussian_2.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="687" data-original-width="800" height="274" src="https://camel.apache.org/blog/Camel3-Whatsnew/camel3-3humps_hu7ba293cf525bac57712ef8f5199c56b4_332440_800x0_resize_q95_gaussian_2.png" width="320" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;This blog post highlights the noteworthy new features and improvements in Camel v3.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;1) CAMEL IS NOW A FAMILY OF PROJECTS&lt;/b&gt;&lt;br /&gt;Apache Camel, is now a family of projects (3 at this time of writing):&lt;br /&gt;&lt;br /&gt;&lt;ul style="text-align: left;"&gt;&lt;li&gt;&lt;a href="https://github.com/apache/camel"&gt;Camel 3&lt;/a&gt;: Integration Framework Swiss knife of integration&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/apache/camel-k/"&gt;Camel K&lt;/a&gt;: Lightweight Serverless Integration Platform Camel on Kubernetes &amp;amp; Knative&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/apache/camel-quarkus"&gt;Camel Quarkus&lt;/a&gt;: Camel extensions for Quarkus Optimised JVM &amp;amp; Native compiled Java (GraalVM)&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://camel.apache.org/blog/Camel3-Whatsnew/camel3-projects_hu14eb14882812af4ef4cf988ec2e12bd3_88043_800x0_resize_q95_gaussian_2.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="338" data-original-width="800" height="168" src="https://camel.apache.org/blog/Camel3-Whatsnew/camel3-projects_hu14eb14882812af4ef4cf988ec2e12bd3_88043_800x0_resize_q95_gaussian_2.png" width="400" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;The Camel code-base is very large, and we have setup sub-projects for new innovative projects using Camel. The first sub-project was to run Camel as cloud-native on Kubernetes in a serverless manner which became Camel K. Then Camel Quarkus came to make Java and Camel with very fast startup and very small memory footprint primary for container based deployments.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;2) NEW WEBSITE&lt;/b&gt;&lt;br /&gt;A major goal for Camel 3 was to finally revamp the old aging website to use modern technologies and be able to auto-generate content from the source code. This has taken years to get to this point as we have built tools over the last many Camel 2.x releases that could take us closer. At end of 2019 then the Camel community and others stepped up and provided the new art-work, logo, and look and feel for the new website - thank you very much!.&lt;br /&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://camel.apache.org/blog/Camel3-Whatsnew/camel3-website.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="372" data-original-width="800" height="185" src="https://camel.apache.org/blog/Camel3-Whatsnew/camel3-website.png" width="400" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;For Camel 3.x we will continue to improve the website and the documentation. This is much easier for us to do, and also for people to contribute changes as its just a regular github PR to provide updates. We love contributions.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;Zoran had some fun with the new look and feel and he added a little gem; if you stare at the front page, then you should see a little animation of the curved bezel ;)&lt;br /&gt;&lt;br /&gt;&lt;b&gt;3) JAVA 11&lt;/b&gt;&lt;br /&gt;Camel 3 is the first official release that supports Java 11. Java 8 will still be supported for the first number of 3.x releases, but is expected to be dropped later in 2020. However we wanted to provide Java 8 support to help migrate Camel 2.x users whom may be restricted to Java 8 for some time to come.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;4) MODULARIZED CAMEL-CORE&lt;/b&gt;&lt;br /&gt;The camel-core has been modularized from 1 JAR to 33 JARs. The core functionality has been splitup into:&lt;br /&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-api&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-base&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-caffeine-lrucache&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-cloud&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-core&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-core-engine&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-core-osgi&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-core-xml&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-endpointdsl&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-headersmap&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-jaxp&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-main&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-management-api&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-management-impl&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-support&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-util&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-util-json&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;For Camel end users then only a few JARs is relevant.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;camel-api&lt;/b&gt; contains the public API for Camel (eg interfaces such as CamelContext, Endpoint, Exchange, Message, and so on).&lt;br /&gt;&lt;br /&gt;&lt;b&gt;camel-support &lt;/b&gt;contains the base classes and RouteBuilder which you would use to build Camel routes and applications. This JAR is also contains necessary base classes for building custom Camel components, and other kinds of plugins.&lt;br /&gt;&lt;br /&gt;The components that resided in camel-core has also be externalized into individual components:&lt;br /&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-bean&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-log&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-stub&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-browse&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-mock&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-timer&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-controlbus&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-properties&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-validator&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-dataformat&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-ref&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-vm&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-direct&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-rest&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-xpath&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-directvm&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-saga&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-xslt&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-file&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-scheduler&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-zip-deflater&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-language&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;camel-seda&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;Camel end users can then pick and choose exactly only what they need, or keep using everything.&lt;br /&gt;&lt;br /&gt;Therefore we have camel-core and camel-core-engine as two starting dependencies. You can use camel-core which gives you all the JARs which is similar to Camel 2.x. When you use camel-core-engine you get the minimum set of JARs that makes a functional Camel.&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://camel.apache.org/blog/Camel3-Whatsnew/camel3-core-vs-engine.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="217" data-original-width="652" height="132" src="https://camel.apache.org/blog/Camel3-Whatsnew/camel3-core-vs-engine.png" width="400" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;b&gt;camel-core&lt;/b&gt; contains 33 JARs and &lt;b&gt;camel-core-engine&lt;/b&gt; contains 12 JARs.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;5) FASTER STARTUP AND LOWER FOOTPRINT&lt;/b&gt;&lt;br /&gt;We have reduced the size of core Camel and the number of classes loaded. For example in Camel 2 about 5200 classes was loaded, which has been reduced to about 4300 loaded classes in Camel 3.&lt;br /&gt;&lt;br /&gt;We have also done many smaller optimizations in the core, to reduce the number of allocated Java objects, and speeup initialization and other means. We have used JVM profiling tools to assist and find the bottlenecks.&lt;br /&gt;&lt;br /&gt;Another area of improvement is to reduce Java reflections. In Camel 2 then all the configuration of Camel components, endpoints, and routes are reflection based. In Camel 3 we have source code generated Java code for configuration that allows us to use direct Java calls instead of reflections.&lt;br /&gt;&lt;br /&gt;Another similar area is Camel’s type converters which in Camel 2 are Java reflection based (you could build custom type converts that were not reflection based). In Camel 3 we also generate Java source code which means that type converting is direct Java calls at runtime.&lt;br /&gt;&lt;br /&gt;We have also moved initialization logic to earlier phases when it was possible. For example there is a new build phase which allows Camel to do special initialization during building your project (this requires Camel Quarkus).&lt;br /&gt;&lt;br /&gt;All this optimization improves the startup performance of Camel and reduces the memory overhead. With Camel Quarkus you can natively compile your Camel application and make it startup in 30 milli seconds and consume only 10mb of memory (RSS) with a full blown HTTP REST server and health-checks and metrics.&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://camel.apache.org/blog/Camel3-Whatsnew/camel3-quarkus_huba2c5d62ef8e8448d234c82a5572ee1a_737648_800x0_resize_q95_gaussian_2.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="595" data-original-width="800" height="297" src="https://camel.apache.org/blog/Camel3-Whatsnew/camel3-quarkus_huba2c5d62ef8e8448d234c82a5572ee1a_737648_800x0_resize_q95_gaussian_2.png" width="400" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;There are still a few items on the agenda that we want to work on in Camel 3.x to further optimize Camel core.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;6) TYPE SAFE ENDPOINT DSL&lt;/b&gt;&lt;br /&gt;Camel end users whom has configured endpoints using URI strings, would all have experienced the problem when you make a configuration mistake in the endpoint, which then makes Camel fail on startup.&lt;br /&gt;&lt;br /&gt;In Camel 3, we have a new type-safe DSL for endpoints which you can use in Java routes. You can continue to use the classic URI strings, but if you want to try the endpoint DSL, then you need to add camel-endpointdsl to your classpath. Then you should extend EndpointRouteBuilder instead of RouteBuilder to access the endpoint DSL.&lt;br /&gt;&lt;br /&gt;Here is a basic example without and with the endpoint DSL:&lt;br /&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;from("timer:click?period=3000&amp;amp;fixedRate=true")&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;&amp;nbsp; &amp;nbsp; .to("seda:foo?blockWhenFull=true");&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;&lt;br /&gt;&lt;/span&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;from(timer("click").period(3000).fixedRate(true))&lt;/span&gt;&lt;br /&gt;&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;&amp;nbsp; &amp;nbsp; .to(seda("foo").blockWhenFull(true));&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;You can also find a &lt;a href="https://github.com/apache/camel/tree/master/examples/camel-example-cafe-endpointdsl"&gt;little example&lt;/a&gt; in the source code.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;7) REACTIVE ROUTING ENGINE&lt;/b&gt;&lt;br /&gt;The routing engine in Camel has internally been reactive’fied and all EIPs has been retrofitted to work in a reactive manner. However this is internal only, and the Camel API for both end users and component developers are based on existing callback behavior.&lt;br /&gt;&lt;br /&gt;We will later introduce and work on a client-side facing reactive API after we have jumped to Java 11 as minimum version (then we can support Java 9 flowable API).&lt;br /&gt;&lt;br /&gt;Camel already have integration with reactive frameworks such as Vert.X, RxJava and Reactor Core in the dedicated Camel components.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;8) CAMEL MAIN&lt;/b&gt;&lt;br /&gt;We have introduced camel-main as a standalone JAR that makes it easier to run just Camel. There are a couple of examples with the source code that demonstrates how to do that.&lt;br /&gt;&lt;br /&gt;We also use camel-main to have common code to configure and bootstrap Camel for standalone, Spring Boot, Camel K, and Camel Quarkus. This allows us to share the same code, and configuration options.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;9) CAMEL MICROPROFILE&lt;/b&gt;&lt;br /&gt;Camel 3 now integrates better with Eclipse Microprofile and we have Camel components for Microprofile configuration, metrics, health checks, and fault tolerance (on the way).&lt;br /&gt;&lt;br /&gt;More components to come in upcoming Camel releases. These microprofile components are also used by Camel Quarkus.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;10) MISCELLANEOUS IMPROVEMENTS&lt;/b&gt;&lt;br /&gt;Camel 3 now supports JUnit 5 for unit tests, with the test components that have -junit5 as suffix.&lt;br /&gt;&lt;br /&gt;The Camel Registry is now also writeable, so you can add beans to the registry at runtime, or from unit tests etc.&lt;br /&gt;&lt;br /&gt;You can also configure endpoints (producer) to lazy start. By default Camel works in a fail-fast mode, which means that Camel components that fails to connect to external systems during startup may cause the route to fail on startup. For Camel 3 you can now configure these endpoints to lazy start, which means the route will startup and they will first fail when a message is routed to the endpoint.&lt;br /&gt;&lt;br /&gt;Camel also allows to configure your routes to be supervised during startup, which allows Camel to more intelligently start routes in a more safe manner, by restarting routes that failed.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;11) MIGRATING TO CAMEL 3&lt;/b&gt;&lt;br /&gt;We have of course cleaned up the code base, such as removing all deprecated APIs and components. We have also adjusted some APIs to make them easier to use from end users, and more Java 8 lambda friendly.&lt;br /&gt;&lt;br /&gt;Internally we have also adjusted the route model, to make it easier to extend into new DSLs; and there is a YAML DSL on the way which was initiated in Camel K.&lt;br /&gt;&lt;br /&gt;In terms of backwards compatibility then Camel 3 is mostly compatibility for regular Camel applications. However if you are using some of the more advanced features and other plugins in Camel then migration is needed. Also custom components must be migrated and recompiled. There are other adjustments such as Spring Boot users must use org.apache.camel.springboot as groupId instead of org.apache.camel etc. All details can be seen in the &lt;a href="https://camel.apache.org/manual/latest/camel-3-migration-guide.html"&gt;migration guide&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;Good luck with your migration if you decide to continue your Camel journey. And for new users to Camel then good luck getting onboard.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;12) BONUS: NEW COMPONENTS&lt;/b&gt;&lt;br /&gt;There are 30 net new components in Camel 3, such as more stuff for Amazon AWS, and with GraphQL, and also worthwhile to mention is integration with Debezium, which is a change data capture project to grab change events from databases.&amp;nbsp;&lt;/div&gt;&lt;div class="feedflare"&gt; &lt;a href="http://feeds.feedburner.com/~ff/ApacheCamel?a=kTqFtzpfqvU:aj7D0skvf-g:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/ApacheCamel?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/ApacheCamel?a=kTqFtzpfqvU:aj7D0skvf-g:4cEx4HpKnUU"&gt;&lt;img src="http://feeds.feedburner.com/~ff/ApacheCamel?i=kTqFtzpfqvU:aj7D0skvf-g:4cEx4HpKnUU" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/ApacheCamel?a=kTqFtzpfqvU:aj7D0skvf-g:F7zBnMyn0Lo"&gt;&lt;img src="http://feeds.feedburner.com/~ff/ApacheCamel?i=kTqFtzpfqvU:aj7D0skvf-g:F7zBnMyn0Lo" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/ApacheCamel?a=kTqFtzpfqvU:aj7D0skvf-g:V_sGLiPBpWU"&gt;&lt;img src="http://feeds.feedburner.com/~ff/ApacheCamel?i=kTqFtzpfqvU:aj7D0skvf-g:V_sGLiPBpWU" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/ApacheCamel/~4/kTqFtzpfqvU" height="1" width="1" alt=""/&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Fa5Om9oz54s" height="1" width="1" alt=""/&gt;</content><summary>Apache Camel 3 was released last thursday November 28th 2019, which also happens to be the day of the US Thanksgiving. This was not intentionally but we can say its a big thanks from us to the community with a brand new major version of Camel - this does not come by often. In fact it's 10 years since Camel 2 hit the streets. So this 3rd generation is long overdue. This blog post highlights the not...</summary><dc:creator>Claus Ibsen</dc:creator><dc:date>2019-12-04T11:01:00Z</dc:date><feedburner:origLink>http://feedproxy.google.com/~r/ApacheCamel/~3/kTqFtzpfqvU/apache-camel-3-whats-new-top-10.html</feedburner:origLink></entry><entry><title>Understanding Red Hat AMQ Streams components for OpenShift and Kubernetes: Part 1</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/hi6Xs2GC_v8/" /><category term="Event-Driven" /><category term="Red Hat AMQ" /><category term="Red Hat Integration" /><category term="Red Hat OpenShift Container Platform" /><category term="Apache Kafka" /><category term="Kafka streams" /><category term="Red Hat AMQ Streams" /><category term="Strimzi" /><author><name>Pramod Padmanabhan</name></author><id>https://developers.redhat.com/blog/?p=650037</id><updated>2019-12-04T08:00:06Z</updated><published>2019-12-04T08:00:06Z</published><content type="html">&lt;p&gt;&lt;a href="https://access.redhat.com/products/red-hat-amq#streams" target="_blank" rel="noopener noreferrer"&gt;Red Hat AMQ Streams&lt;/a&gt; is an enterprise-grade &lt;a href="https://kafka.apache.org/" target="_blank" rel="noopener noreferrer"&gt;Apache Kafka&lt;/a&gt; (event streaming) solution, which enables systems to exchange data at high throughput and low latency. AMQ Streams is available as part of the Red Hat AMQ offering in two different flavors: one on the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.5/html/using_amq_streams_on_red_hat_enterprise_linux_rhel/" target="_blank" rel="noopener noreferrer"&gt;Red Hat Enterprise Linux platform&lt;/a&gt; and another on the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.5/html/using_amq_streams_on_openshift_container_platform/" target="_blank" rel="noopener noreferrer"&gt;OpenShift Container Platform&lt;/a&gt;. In this three-part article series, we will cover AMQ Streams on the OpenShift Container Platform.&lt;/p&gt; &lt;p&gt;To get the most out of these articles, it will help to be familiar with messaging concepts, &lt;a href="https://developers.redhat.com/openshift/"&gt;Red Hat OpenShift&lt;/a&gt;, and &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;span id="more-650037"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;When running on containers, AMQ Streams poses different challenges (see this talk by &lt;a href="https://www.youtube.com/watch?v=rzHQvImn2XY" target="_blank" rel="noopener noreferrer"&gt;Sean Glover&lt;/a&gt;), such as:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Upgrading Kafka&lt;/li&gt; &lt;li&gt;Beginning deployment&lt;/li&gt; &lt;li&gt;Managing ZooKeeper&lt;/li&gt; &lt;li&gt;Replacing brokers&lt;/li&gt; &lt;li&gt;Rebalancing topic partitions&lt;/li&gt; &lt;li&gt;Decommissioning or adding brokers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These challenges are resolved using the &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/operator/" target="_blank" rel="noopener noreferrer"&gt;Operator pattern&lt;/a&gt; from the &lt;a href="https://strimzi.io/" target="_blank" rel="noopener noreferrer"&gt;Strimzi&lt;/a&gt; project.&lt;/p&gt; &lt;p&gt;Now that we have a basic background for Red Hat AMQ Streams, let&amp;#8217;s dive into how it all works.&lt;/p&gt; &lt;h2&gt;Red Hat AMQ Streams deep dive&lt;/h2&gt; &lt;p&gt;AMQ Streams has multiple Operators, which helps in solving the challenges of running AMQ Streams in the container world:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Cluster Operator&lt;/strong&gt;: Deploys and manages Kafka clusters on Enterprise containers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Entity Operator&lt;/strong&gt;: Manages users and topics using two different sub-operators. The &lt;strong&gt;Topic Operator&lt;/strong&gt; manages Kafka topics in your Kafka cluster, and the &lt;strong&gt;User Operator&lt;/strong&gt; manages Kafka users on your Kafka cluster.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Kafka Connect&lt;/strong&gt;: Connects external systems to the Kafka cluster.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Kafka Mirror Maker&lt;/strong&gt;: Replicates data between Kafka clusters.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Kafka Bridge&lt;/strong&gt;: Acts as a bridge between different protocols and Kafka clusters. Currently supports HTTP 1.1 and AMQP 1.0.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Figure 1 shows a bird&amp;#8217;s view of Red Hat AMQ Streams on Red Hat OpenShift:&lt;/p&gt; &lt;div id="attachment_653677" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcca436f1380.png"&gt;&lt;img aria-describedby="caption-attachment-653677" class="wp-image-653677 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcca436f1380-1024x559.png" alt="AMQ Stream reference design on openshift, kubernetes. Enterprise Apache Kafka. Enterprise Strimzi" width="640" height="349" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcca436f1380-1024x559.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcca436f1380-300x164.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dcca436f1380-768x420.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-653677" class="wp-caption-text"&gt;Figure 1: How Red Hat AMQ Streams and Red Hat OpenShift interact.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Now let&amp;#8217;s create a &amp;#8220;hello world&amp;#8221; program for all of these components. Due to the size of this walk-through, we will cover this topic in three articles, as follows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Part 1: Setting up ZooKeeper, Kafka, and the Entity Operator&lt;/li&gt; &lt;li&gt;Part 2: Kafka Connect, Kafka Bridge, and Mirror Maker&lt;/li&gt; &lt;li&gt;Part 3: Monitoring and administration&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Setting up ZooKeeper, Kafka, and the Entity Operator&lt;/h2&gt; &lt;p&gt;Before starting, you will need an OCP cluster with a Red Hat subscription to access the Red Hat container images, and cluster admin access. This walk-through uses Red Hat AMQ Streams 1.3.0:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Download and extract Red Hat AMQ Streams 1.3.0 and the OpenShift Container Platform Images from the &lt;a href="https://access.redhat.com/announcements/4529551" target="_blank" rel="noopener noreferrer"&gt;Red Hat AMQ Streams 1.3.o download page&lt;/a&gt;:&lt;/li&gt; &lt;/ol&gt; &lt;pre style="padding-left: 40px;"&gt;$ unzip amq-streams-1.3.0-ocp-install-examples.zip&lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;There will be two folders: &lt;code&gt;examples&lt;/code&gt; and &lt;code&gt;install&lt;/code&gt;.&lt;/p&gt; &lt;ol start="2"&gt; &lt;li&gt;Log in and create a new project and namespace for AMQ Streams (see Figure 2):&lt;/li&gt; &lt;/ol&gt; &lt;pre style="padding-left: 40px;"&gt;$ oc login -u admin_user -p admin_password https://redhat.ocp.cluster.url.com $ oc new-project amq-streams&lt;/pre&gt; &lt;div id="attachment_651567" style="width: 471px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dc8b39a0ac6a.png"&gt;&lt;img aria-describedby="caption-attachment-651567" class="wp-image-651567 size-full" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dc8b39a0ac6a.png" alt="amq-streams project creation image" width="461" height="216" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dc8b39a0ac6a.png 461w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dc8b39a0ac6a-300x141.png 300w" sizes="(max-width: 461px) 100vw, 461px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-651567" class="wp-caption-text"&gt;Figure 2: AMQ Streams now shows up as a project.&lt;/p&gt;&lt;/div&gt; &lt;ol start="3"&gt; &lt;li&gt;Navigate to the &lt;code&gt;install/cluster-operator&lt;/code&gt; folder and modify the role-binding YAML files to use &lt;code&gt;amq-streams&lt;/code&gt; as their namespace:&lt;/li&gt; &lt;/ol&gt; &lt;pre style="padding-left: 40px;"&gt;$ sed -i 's/namespace: .*/namespace: amq-streams/' install/cluster-operator/*RoleBinding*.yaml&lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;For macOS:&lt;/p&gt; &lt;pre style="padding-left: 40px;"&gt;$ sed -i '' 's/namespace: .*/namespace: amq-streams/' install/cluster-operator/*RoleBinding*.yaml&lt;/pre&gt; &lt;ol start="4"&gt; &lt;li&gt;Create the Cluster Operator (see Figure 3):&lt;/li&gt; &lt;/ol&gt; &lt;pre style="padding-left: 40px;"&gt;$ oc apply -f install/cluster-operator kafka_2.12-2.3.0.redhat-00003 pramod$ oc apply -f install/cluster-operator serviceaccount/strimzi-cluster-operator created clusterrole.rbac.authorization.k8s.io/strimzi-cluster-operator-namespaced created rolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator created clusterrole.rbac.authorization.k8s.io/strimzi-cluster-operator-global created clusterrolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator created clusterrole.rbac.authorization.k8s.io/strimzi-kafka-broker created clusterrolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator-kafka-broker-delegation created clusterrole.rbac.authorization.k8s.io/strimzi-entity-operator created rolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator-entity-operator-delegation created clusterrole.rbac.authorization.k8s.io/strimzi-topic-operator created rolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator-topic-operator-delegation created customresourcedefinition.apiextensions.k8s.io/kafkas.kafka.strimzi.io created customresourcedefinition.apiextensions.k8s.io/kafkaconnects.kafka.strimzi.io created customresourcedefinition.apiextensions.k8s.io/kafkaconnects2is.kafka.strimzi.io created customresourcedefinition.apiextensions.k8s.io/kafkatopics.kafka.strimzi.io created customresourcedefinition.apiextensions.k8s.io/kafkausers.kafka.strimzi.io created customresourcedefinition.apiextensions.k8s.io/kafkamirrormakers.kafka.strimzi.io created customresourcedefinition.apiextensions.k8s.io/kafkabridges.kafka.strimzi.io created deployment.apps/strimzi-cluster-operator created&lt;/pre&gt; &lt;div id="attachment_651577" style="width: 510px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dc8d869524a2.png"&gt;&lt;img aria-describedby="caption-attachment-651577" class="wp-image-651577" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dc8d869524a2-300x86.png" alt="AMQ stream cluster operator" width="500" height="144" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dc8d869524a2-300x86.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dc8d869524a2-768x221.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dc8d869524a2.png 945w" sizes="(max-width: 500px) 100vw, 500px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-651577" class="wp-caption-text"&gt;Figure 3: The strimzi-cluster-operator was created.&lt;/p&gt;&lt;/div&gt; &lt;ol start="5"&gt; &lt;li&gt;Ensure you have eight physical volumes. For the walk-through, we are using 5GB persistent volumes:&lt;/li&gt; &lt;/ol&gt; &lt;pre style="padding-left: 40px;"&gt;$ oc get pv | grep Available kafka_2.12-2.3.0.redhat-00003 pramod$ oc get pv -o wide | grep Available NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pv14 5Gi RWO Recycle Available 34m pv19 5Gi RWO Recycle Available 34m pv20 5Gi RWO Recycle Available 34m pv21 5Gi RWO Recycle Available 34m pv23 5Gi RWO Recycle Available 34m pv3 5Gi RWO Recycle Available 34m pv5 5Gi RWO Recycle Available 34m pv9 5Gi RWO Recycle Available 34m&lt;/pre&gt; &lt;ol start="6"&gt; &lt;li&gt;Create the persistent cluster config &lt;code&gt;amq-kafka-cluster.yml&lt;/code&gt;. The example file present in &lt;code&gt;examples/kafka/kafka-persistent.yml&lt;/code&gt; was used as a reference for this config:&lt;/li&gt; &lt;/ol&gt; &lt;pre style="padding-left: 40px;"&gt;apiVersion: kafka.strimzi.io/v1beta1 kind: Kafka metadata: name: simple-cluster spec: kafka: version: 2.3.0 replicas: 5 listeners: plain: {} tls: {} config: offsets.topic.replication.factor: 5 transaction.state.log.replication.factor: 5 transaction.state.log.min.isr: 2 log.message.format.version: "2.3" storage: type: jbod volumes: - id: 0 type: persistent-claim size: 5Gi deleteClaim: false zookeeper: replicas: 3 storage: type: persistent-claim size: 5Gi deleteClaim: false entityOperator: topicOperator: {} userOperator: {}&lt;/pre&gt; &lt;ol start="7"&gt; &lt;li&gt;Create the AMQ Streams cluster (see Figure 4):&lt;/li&gt; &lt;/ol&gt; &lt;pre style="padding-left: 40px;"&gt;$ oc apply -f amq-kafka-cluster.yml pramod$ oc get pv | grep Bound pv12 5Gi RWO Recycle Bound ocplab/mongodb 38m pv14 5Gi RWO Recycle Bound amq-streams/data-0-simple-cluster-kafka-4 38m pv19 5Gi RWO Recycle Bound amq-streams/data-0-simple-cluster-kafka-3 38m pv20 5Gi RWO Recycle Bound amq-streams/data-0-simple-cluster-kafka-2 38m pv21 5Gi RWO Recycle Bound amq-streams/data-0-simple-cluster-kafka-1 38m pv23 5Gi RWO Recycle Bound amq-streams/data-0-simple-cluster-kafka-0 38m pv3 5Gi RWO Recycle Bound amq-streams/data-simple-cluster-zookeeper-2 38m pv5 5Gi RWO Recycle Bound amq-streams/data-simple-cluster-zookeeper-0 38m pv9 5Gi RWO Recycle Bound amq-streams/data-simple-cluster-zookeeper-3 38m&lt;/pre&gt; &lt;div id="attachment_651627" style="width: 510px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dc8e3b3c15d3.png"&gt;&lt;img aria-describedby="caption-attachment-651627" class="wp-image-651627" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dc8e3b3c15d3-300x126.png" alt="AMQ stream cluster" width="500" height="211" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dc8e3b3c15d3-300x126.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dc8e3b3c15d3.png 724w" sizes="(max-width: 500px) 100vw, 500px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-651627" class="wp-caption-text"&gt;Figure 4: ZooKeeper, Kafka, and the Entity operator now appear.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;To test your cluster:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Log into the OpenShift Container Platform (OCP) cluster, create a producer sample app, and push a few messages:&lt;/li&gt; &lt;/ol&gt; &lt;pre style="padding-left: 40px;"&gt;$ oc run kafka-producer -ti --image=registry.redhat.io/amq7/amq-streams-kafka-23:1.3.0 --rm=true --restart=Never -- bin/kafka-console-producer.sh --broker-list simple-cluster-kafka-bootstrap:9092 --topic redhat-demo-topics If you don't see a command prompt, try pressing enter. &amp;#62;hello world &amp;#62;from pramod&lt;/pre&gt; &lt;p id="gBwjOPc" style="padding-left: 40px;"&gt;Ignore the warning for now.&lt;/p&gt; &lt;ol start="2"&gt; &lt;li&gt;Open another terminal and create a consumer sample app to listen to the messages:&lt;/li&gt; &lt;/ol&gt; &lt;pre style="padding-left: 40px;"&gt;$ oc run kafka-consumer -ti --image=registry.redhat.io/amq7/amq-streams-kafka-23:1.3.0 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server simple-cluster-kafka-bootstrap:9092 --topic redhat-demo-topics --from-beginning&lt;/pre&gt; &lt;p style="padding-left: 40px;"&gt;You should see two messages, which were published using the producer terminal, as shown in Figure 5:&lt;/p&gt; &lt;div id="attachment_652187" style="width: 650px" class="wp-caption aligncenter"&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dc9f71d54ebe.png"&gt;&lt;img aria-describedby="caption-attachment-652187" class="wp-image-652187 size-large" src="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dc9f71d54ebe-1024x99.png" alt="" width="640" height="62" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dc9f71d54ebe-1024x99.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dc9f71d54ebe-300x29.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2019/11/img_5dc9f71d54ebe-768x74.png 768w" sizes="(max-width: 640px) 100vw, 640px" /&gt;&lt;/a&gt;&lt;p id="caption-attachment-652187" class="wp-caption-text"&gt;Figure 5: The consumer terminal listening to the producer&amp;#8217;s messages.&lt;/p&gt;&lt;/div&gt; &lt;ol start="3"&gt; &lt;li&gt;Exit from both the producer and the consumer connections using Ctrl+C.&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this article, we explored Red Hat AMQ Streams basics and its components. We also showed how to create a basic Red Hat AMQ cluster on Red Hat OpenShift. In the next article, we will address Kafka Connect, the Kafka Bridge, and Mirror Maker.&lt;/p&gt; &lt;h3&gt;References&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/pramodmax/amqstream-getting-started" target="_blank" rel="noopener noreferrer"&gt;GitHub for the snippet&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://access.redhat.com/products/red-hat-amq#streams" target="_blank" rel="noopener noreferrer"&gt;Red Hat AMQ Streams 1.3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://strimzi.io/" target="_blank" rel="noopener noreferrer"&gt;Strimzi&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=rzHQvImn2XY" target="_blank" rel="noopener noreferrer"&gt;Running Kafka On Kubernetes With Strimzi For Real-Time Streaming Applications&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.5/html-single/using_amq_streams_on_openshift/index" target="_blank" rel="noopener noreferrer"&gt;Using AMQ Streams on OpenShift&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F04%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-1%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%201" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F04%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-1%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%201" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F04%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-1%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%201" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F04%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-1%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%201" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F04%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-1%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%201" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F04%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-1%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%201" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F04%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-1%2F&amp;#38;linkname=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%201" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F04%2Funderstanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-1%2F&amp;#038;title=Understanding%20Red%20Hat%20AMQ%20Streams%20components%20for%20OpenShift%20and%20Kubernetes%3A%20Part%201" data-a2a-url="https://developers.redhat.com/blog/2019/12/04/understanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-1/" data-a2a-title="Understanding Red Hat AMQ Streams components for OpenShift and Kubernetes: Part 1"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/12/04/understanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-1/"&gt;Understanding Red Hat AMQ Streams components for OpenShift and Kubernetes: Part 1&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/hi6Xs2GC_v8" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Red Hat AMQ Streams is an enterprise-grade Apache Kafka (event streaming) solution, which enables systems to exchange data at high throughput and low latency. AMQ Streams is available as part of the Red Hat AMQ offering in two different flavors: one on the Red Hat Enterprise Linux platform and another on the OpenShift Container Platform. In this [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/12/04/understanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-1/"&gt;Understanding Red Hat AMQ Streams components for OpenShift and Kubernetes: Part 1&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><post-id xmlns="com-wordpress:feed-additions:1">650037</post-id><dc:creator>Pramod Padmanabhan</dc:creator><dc:date>2019-12-04T08:00:06Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/12/04/understanding-red-hat-amq-streams-components-for-openshift-and-kubernetes-part-1/</feedburner:origLink></entry><entry><title>APIs as a Product: Get started in no time</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/A0tpRzNu2Z0/" /><category term="Red Hat 3scale API Management" /><category term="Red Hat Integration" /><category term="API" /><category term="API as a Product" /><author><name>Nicolas Massé</name></author><id>https://developers.redhat.com/blog/?p=657237</id><updated>2019-12-03T08:00:27Z</updated><published>2019-12-03T08:00:27Z</published><content type="html">&lt;p&gt;In the previous article, &lt;a href="https://developers.redhat.com/blog/2019/11/21/apis-as-a-product/"&gt;APIs as a Product: Get the value out of your APIs&lt;/a&gt;, we presented a new approach called &amp;#8220;APIs as a Product&amp;#8221; to maximize the value of your APIs. In this article, we show how to quickly get started with APIs as a Product using the new features of &lt;a href="https://developers.redhat.com/products/3scale/overview"&gt;Red Hat 3scale API Management 2.7&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;To showcase the power of 3scale 2.7&amp;#8217;s new features, combined with the awesomeness of the open source communities &lt;a href="https://www.apicur.io/" target="_blank" rel="noopener noreferrer"&gt;Apicurio&lt;/a&gt; and &lt;a href="http://microcks.github.io/" target="_blank" rel="noopener noreferrer"&gt;Microcks&lt;/a&gt;, we will design two APIs as a Product and show how we can compose both products in 3scale to get the resulting API as a Product.&lt;/p&gt; &lt;p&gt;&lt;span style="font-weight: 400;"&gt;Let’s look at the well-known Petstore example. &lt;/span&gt;&lt;span style="font-weight: 400;"&gt;Imagine for a moment that the first steps of the API Design Thinking process led to this rough definition of the customer’s needs:&lt;/span&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;span style="font-weight: 400;"&gt;Petstore is a company selling cuddly toys online. They would like to open a marketplace to let partners resell the pets. Ideally, orders have to placed through an API so that the whole process can be automated. The partners are split over what they want:&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span style="font-weight: 400;"&gt;One group of Petstore&amp;#8217;s partners expressed the need to be able to discover the Petstore inventory to add items to their own catalog.&lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span style="font-weight: 400;"&gt;Other partners want to have the checkout process managed for them, from cart creation up to checkout. &lt;/span&gt;&lt;/li&gt; &lt;li&gt;&lt;span style="font-weight: 400;"&gt;A last group of partners wants to do both.&lt;/span&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;span style="font-weight: 400;"&gt;In the first part of the video below, we go through the API Ideation process to design two products: One&lt;/span&gt;&lt;span style="font-weight: 400;"&gt; to discover the inventory, and one &lt;/span&gt;to place an order. &lt;span style="font-weight: 400;"&gt;Then, we&amp;#8217;ll go through the API Prototyping step to expose our two products as live mocks.&lt;/span&gt;&lt;/p&gt; &lt;p&gt;&lt;span style="font-weight: 400;"&gt;Finally, we use the features of 3scale 2.7 to compose both products as a unique product, expose it through API Management, and gather feedback from our early adopters.&lt;/span&gt;&lt;/p&gt; &lt;p&gt;To get started with APIs as a Product, watch this video:&lt;/p&gt; &lt;p&gt;&lt;iframe class='youtube-player' type='text/html' width='640' height='360' src='https://www.youtube.com/embed/-H4bh1UbZNc?version=3&amp;#038;rel=1&amp;#038;fs=1&amp;#038;autohide=2&amp;#038;showsearch=0&amp;#038;showinfo=1&amp;#038;iv_load_policy=1&amp;#038;wmode=transparent' allowfullscreen='true' style='border:0;'&gt;&lt;/iframe&gt;&lt;/p&gt; &lt;p&gt;Note that:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;API Ideation starts at 01:37.&lt;/li&gt; &lt;li&gt;API Prototyping starts at 08:53.&lt;/li&gt; &lt;li&gt;The API as a Product features of 3scale 2.7 are showcased at 18:46.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F03%2Fapis-as-a-product-get-started-in-no-time%2F&amp;#38;linkname=APIs%20as%20a%20Product%3A%20Get%20started%20in%20no%20time" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F03%2Fapis-as-a-product-get-started-in-no-time%2F&amp;#38;linkname=APIs%20as%20a%20Product%3A%20Get%20started%20in%20no%20time" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F03%2Fapis-as-a-product-get-started-in-no-time%2F&amp;#38;linkname=APIs%20as%20a%20Product%3A%20Get%20started%20in%20no%20time" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F03%2Fapis-as-a-product-get-started-in-no-time%2F&amp;#38;linkname=APIs%20as%20a%20Product%3A%20Get%20started%20in%20no%20time" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F03%2Fapis-as-a-product-get-started-in-no-time%2F&amp;#38;linkname=APIs%20as%20a%20Product%3A%20Get%20started%20in%20no%20time" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F03%2Fapis-as-a-product-get-started-in-no-time%2F&amp;#38;linkname=APIs%20as%20a%20Product%3A%20Get%20started%20in%20no%20time" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F03%2Fapis-as-a-product-get-started-in-no-time%2F&amp;#38;linkname=APIs%20as%20a%20Product%3A%20Get%20started%20in%20no%20time" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F03%2Fapis-as-a-product-get-started-in-no-time%2F&amp;#038;title=APIs%20as%20a%20Product%3A%20Get%20started%20in%20no%20time" data-a2a-url="https://developers.redhat.com/blog/2019/12/03/apis-as-a-product-get-started-in-no-time/" data-a2a-title="APIs as a Product: Get started in no time"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/12/03/apis-as-a-product-get-started-in-no-time/"&gt;APIs as a Product: Get started in no time&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/A0tpRzNu2Z0" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;In the previous article, APIs as a Product: Get the value out of your APIs, we presented a new approach called &amp;#8220;APIs as a Product&amp;#8221; to maximize the value of your APIs. In this article, we show how to quickly get started with APIs as a Product using the new features of Red Hat 3scale [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/12/03/apis-as-a-product-get-started-in-no-time/"&gt;APIs as a Product: Get started in no time&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><post-id xmlns="com-wordpress:feed-additions:1">657237</post-id><dc:creator>Nicolas Massé</dc:creator><dc:date>2019-12-03T08:00:27Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/12/03/apis-as-a-product-get-started-in-no-time/</feedburner:origLink></entry><entry><title>Red Hat CodeReady Workspaces 2: New tools to speed Kubernetes development</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/1_gMQV7wJFo/" /><category term="Announcement" /><category term="Red Hat CodeReady Workspaces" /><category term="containers" /><category term="Kubernetes" /><author><name>Stevan LeMeur</name></author><id>https://developers.redhat.com/blog/?p=660297</id><updated>2019-12-03T08:00:21Z</updated><published>2019-12-03T08:00:21Z</published><content type="html">&lt;p&gt;We are pleased to announce the release of &lt;a href="https://developers.redhat.com/products/codeready-workspaces/overview"&gt;Red Hat CodeReady Workspaces 2.0&lt;/a&gt;. Based on &lt;a href="https://www.eclipse.org/che/getting-started/cloud/?sc_cid=701f2000000RtqCAAS"&gt;Eclipse Che&lt;/a&gt;, its upstream project CodeReady Workspaces is a &lt;a href="https://developers.redhat.com/openshift/"&gt;Red Hat OpenShift&lt;/a&gt;-native developer environment enabling cloud-native development for developer teams.&lt;/p&gt; &lt;p&gt;CodeReady Workspaces 2.0 is available now on OpenShift 3.11 and OpenShift 4.x.&lt;/p&gt; &lt;p&gt;This new version introduces:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;b&gt;Kubernetes-native developer sandboxes on OpenShift:&lt;/b&gt; Bring your Kubernetes application into your development environment, allowing you to code, build, test, and run as in production.&lt;/li&gt; &lt;li&gt;&lt;b&gt;Integrated OpenShift experience:&lt;/b&gt; OpenShift plugin and integration into the OpenShift 4 Developer Console.&lt;/li&gt; &lt;li&gt;&lt;b&gt;New editor and Visual Studio (VS) Code extensions compatibility:&lt;/b&gt; New browser-based editor, providing a fast desktop-like experience and compatibility with Visual Studio Code extensions.&lt;/li&gt; &lt;li&gt;&lt;b&gt;Devfile, developer environment as code:&lt;/b&gt; Developer environments are codified with a devfile making them consistent, repeatable, and reproducible.&lt;/li&gt; &lt;li&gt;&lt;b&gt;Centrally hosted on OpenShift with AirGap:&lt;/b&gt; Deploy on your OpenShift cluster, behind your firewall. AirGap capabilities. Easier to monitor and administer.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;span id="more-660297"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;Kubernetes-native developer sandboxes on OpenShift&lt;/h2&gt; &lt;p&gt;CodeReady Workspaces 2.0 provides developer environments that are &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt; pods running on OpenShift. In those sandboxes are the components and tools needed to code on a project: a browser-based editor, the plugins, the tools, and the different runtimes required for your project. Everything is running in containers, providing highly consistent, repeatable, and reproducible developer environments—zero config needed, zero pain.&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter wp-image-660327 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/12/crw_v2_f1.png" src="https://developers.redhat.com/blog/wp-content/uploads/2019/12/crw_v2_f1.png" alt="CodeReady Workspaces 2.0, provides developer environments which are kubernetes pods running on OpenShift. " width="512" height="174" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/12/crw_v2_f1.png 512w, https://developers.redhat.com/blog/wp-content/uploads/2019/12/crw_v2_f1-300x102.png 300w" sizes="(max-width: 512px) 100vw, 512px" /&gt;&lt;/p&gt; &lt;p&gt;In this new version, everything you need in your developer environment is fully containerized, even the editor and its plugins. CodeReady Workspaces isolates the tools from your application runtime, allowing you to bring your Kubernetes application into your developer sandbox, code on your application, and run it as in production. The tools are running in sidecar containers and packaged with their own dependencies, making the configuration of your developer environment smooth and keeping your application runtimes untainted from the tools you are using.&lt;/p&gt; &lt;div id="attachment_660357" style="width: 522px" class="wp-caption aligncenter"&gt;&lt;img aria-describedby="caption-attachment-660357" class=" aligncenter wp-image-660357 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/12/crw_v2_f2.png" src="https://developers.redhat.com/blog/wp-content/uploads/2019/12/crw_v2_f2.png" alt="" width="512" height="490" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/12/crw_v2_f2.png 512w, https://developers.redhat.com/blog/wp-content/uploads/2019/12/crw_v2_f2-300x287.png 300w" sizes="(max-width: 512px) 100vw, 512px" /&gt;&lt;p id="caption-attachment-660357" class="wp-caption-text"&gt;Developer tools are containerized, zero dependency installation, isolated execution and lifecycle, easy to upgrade and switch.&lt;/p&gt;&lt;/div&gt; &lt;p&gt;With CodeReady Workspaces, you can code directly on OpenShift, bring your Kubernetes application into your developer sandbox, and code and run your code as close as possible to how it will run on production.&lt;/p&gt; &lt;h2&gt;Integrated OpenShift experience&lt;/h2&gt; &lt;p&gt;CodeReady Workspaces 2.0 enables developers to create applications running on OpenShift easily. Developers benefit from an integrated experience, providing a fast turnaround in the inner-loop development process.&lt;/p&gt; &lt;p&gt;An OpenShift plugin is available to speed up OpenShift development. Developers can connect to any OpenShift cluster and create, debug, and deploy directly from CodeReady Workspaces.&lt;/p&gt; &lt;p&gt;This plugin leverages &lt;a href="https://developers.redhat.com/blog/2019/08/14/openshift-development-with-interactive-odo/"&gt;OpenShift Do (odo)&lt;/a&gt; to simplify inner-loop development and &lt;a href="https://developers.redhat.com/blog/2019/05/03/announcing-odo-developer-focused-cli-for-red-hat-openshift/"&gt;OpenShift CLI (oc)&lt;/a&gt; to help you interact with the OpenShift instance and enriching the inner-loop experience. It is compatible with OpenShift instances (3.11 or 4.x) and supporting public cloud instances, such as Red Hat OpenShift on Azure and AWS.&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter wp-image-660367 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/12/crw_v2_f3.gif" src="https://developers.redhat.com/blog/wp-content/uploads/2019/12/crw_v2_f3.gif" alt="Workspace animated demo" width="512" height="341" /&gt;&lt;/p&gt; &lt;p&gt;Starting with OpenShift 4.2, when CodeReady Workspaces is installed from the Operator Hub, it becomes available in the OpenShift Developer Perspective, allowing you to get a developer workspace from the application topology view.&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter wp-image-660387 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/12/crw_v2_f4.gif" src="https://developers.redhat.com/blog/wp-content/uploads/2019/12/crw_v2_f4.gif" alt="a developer workspace from the application topology view." width="512" height="279" /&gt;&lt;/p&gt; &lt;h2&gt;New editor and VS Code extension compatibility&lt;/h2&gt; &lt;p&gt;CodeReady Workspaces 2.0 provides a new default editor based on &lt;a href="https://theia-ide.org/"&gt;Eclipse Theia&lt;/a&gt;. Developers can enjoy a responsive, desktop-like experience that feels familiar.&lt;/p&gt; &lt;p&gt;This new default editor also brings compatibility with VS Code extensions. The plugin registry provided with CodeReady Workspaces already package tools for the most popular languages: JavaScript, Java, Python, .NET, Go, PHP, XML, and Yaml. You’ll also find tools for OpenShift. Bringing an existing plugin from VS Code is simple; the main difference is in the way the plugins are packaged. On CodeReady Workspaces, plugins are delivered with their dependencies in their own container and running as a sidecar in the workspace pod.&lt;/p&gt; &lt;p&gt;With the new version of CodeReady Workspaces, developers benefit from a richer editing experience with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;b&gt;Command palette:&lt;/b&gt; Full keyboard navigation.&lt;/li&gt; &lt;li&gt;&lt;b&gt;Rich editor:&lt;/b&gt; Find/Replace instances, peek definition, outline view.&lt;/li&gt; &lt;li&gt;&lt;b&gt;Debugger:&lt;/b&gt; Integrated debugging experience.&lt;/li&gt; &lt;li&gt;&lt;b&gt;Git:&lt;/b&gt; Native Git experience, visual indicators for changes in project explorer and editor.&lt;/li&gt; &lt;li&gt;&lt;b&gt;Layout customization:&lt;/b&gt; Configure the layout with drag &amp;#38; drop.&lt;/li&gt; &lt;li&gt;&lt;b&gt;Theming:&lt;/b&gt; White or black themes are available out of the box.&lt;/li&gt; &lt;li&gt;&lt;b&gt;Security:&lt;/b&gt; Secured communication between the editor, the end user, and the workspace.&lt;/li&gt; &lt;li&gt;&lt;b&gt;Extended tasks:&lt;/b&gt; Handles the CodeReady Workspaces commands and provides the ability to start those into a specific container of the workspace.&lt;/li&gt; &lt;li&gt;&lt;b&gt;Extended terminal:&lt;/b&gt; Allows providing a terminal for any of the containers of the workspace pod.&lt;/li&gt; &lt;li&gt;&lt;b&gt;Workspace plugin:&lt;/b&gt; Provides a view that shows all the containers that are running in the workspace and allows interacting with them.&lt;/li&gt; &lt;li&gt;&lt;b&gt;Ports plugin:&lt;/b&gt; Allows detecting when services are running inside of the workspace and automatically expose them.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Air-gapped capabilities&lt;/h2&gt; &lt;p&gt;CodeReady Workspaces supports installation in a restricted environment. This includes your own private cloud, disconnected from the public internet.&lt;/p&gt; &lt;p&gt;CodeReady Workspaces can be configured to rely on your private image registry. The different components needed for CodeReady Workspaces will be installed and configured so that they use only your internal resources.&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter wp-image-660407 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/12/crw_v2_f6.png" src="https://developers.redhat.com/blog/wp-content/uploads/2019/12/crw_v2_f6.png" alt="CodeReady Workspaces can be configured to rely on your private image registry." width="497" height="512" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/12/crw_v2_f6.png 497w, https://developers.redhat.com/blog/wp-content/uploads/2019/12/crw_v2_f6-291x300.png 291w" sizes="(max-width: 497px) 100vw, 497px" /&gt;&lt;/p&gt; &lt;p&gt;The workspaces pod, with the tools and the different plugins, also can be configured to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Leverage your private Git repositories.&lt;/li&gt; &lt;li&gt;Leverage your runtime containers from your own container registries.&lt;/li&gt; &lt;li&gt;Rely on your own maven, npm, or any dependency repository you have.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For installation, follow the documentation instructions for &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_codeready_workspaces/1.2/html/administration_guide/installing_codeready-workspaces"&gt;OpenShift Container Platform 3.11&lt;/a&gt; and &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_codeready_workspaces/2.0/html/installation_guide/installing-codeready-workspaces-on-ocp-4_crw"&gt;OpenShift Container Platform 4.x&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Devfile, developer environment as code&lt;/h2&gt; &lt;p&gt;The devfile provides easy to configure, reproducible definitions of portable developer environments.&lt;/p&gt; &lt;p&gt;The devfile is a declarative abstraction of a developer workspaces, which includes the runtime environments of the application, the source code of the projects mapped to repositories and the tools, plugins, and commands needed to code, build, test, run, and debug an application. This makes the developer workspace replicable. You can use your OpenShift application definition with your devfile; just “dev-mode” it by supercharging the tools you need to code on it.&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter wp-image-660417 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2019/12/crw_v2_f7.png" src="https://developers.redhat.com/blog/wp-content/uploads/2019/12/crw_v2_f7.png" alt="You can use your OpenShift application definition with your devfile, just “dev-mode” it by supercharging the tools you need to code on it." width="512" height="304" srcset="https://developers.redhat.com/blog/wp-content/uploads/2019/12/crw_v2_f7.png 512w, https://developers.redhat.com/blog/wp-content/uploads/2019/12/crw_v2_f7-300x178.png 300w" sizes="(max-width: 512px) 100vw, 512px" /&gt;&lt;br /&gt; Once you have a devfile configured for your project, you can host it on your source code repository.&lt;/p&gt; &lt;p&gt;With the devfile definition and CodeReady Workspaces, developer environments are becoming fully codified, and easily can be modified, shared, forked, and extended. You don’t need to mess with hard-to-maintain and hard-to-manage documentation, VMs, and dockerfiles, which provide only a partial solution on setting up a developer environment.&lt;/p&gt; &lt;h2&gt;Try CodeReady Workspaces 2.0 now&lt;/h2&gt; &lt;p&gt;CodeReady Workspaces 2.0 is available now on OpenShift 3.11 and OpenShift 4.x:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;See the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_codeready_workspaces/2.0/html/installation_guide/installing-codeready-workspaces-on-openshift-3-using-the-operator_crw"&gt;OpenShift 3.11 installation instruction&lt;/a&gt;s.&lt;/li&gt; &lt;li&gt;On OpenShift 4.x, you can install directly from Operator Hub and &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_codeready_workspaces/2.0/html/installation_guide/installing-codeready-workspaces-on-ocp-4_crw"&gt;follow the documentation&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Download the &lt;a href="https://developers.redhat.com/products/codeready-workspaces/download"&gt;Red Hat CodeReady Workspaces CLI&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Visit the &lt;a href="https://developers.redhat.com/products/codeready-workspaces"&gt;Red Hat CodeReady Workspaces product page&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F03%2Fred-hat-codeready-workspaces-2-new-tools-to-speed-kubernetes-development%2F&amp;#38;linkname=Red%20Hat%20CodeReady%20Workspaces%202%3A%20New%20tools%20to%20speed%20Kubernetes%20development" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F03%2Fred-hat-codeready-workspaces-2-new-tools-to-speed-kubernetes-development%2F&amp;#38;linkname=Red%20Hat%20CodeReady%20Workspaces%202%3A%20New%20tools%20to%20speed%20Kubernetes%20development" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F03%2Fred-hat-codeready-workspaces-2-new-tools-to-speed-kubernetes-development%2F&amp;#38;linkname=Red%20Hat%20CodeReady%20Workspaces%202%3A%20New%20tools%20to%20speed%20Kubernetes%20development" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F03%2Fred-hat-codeready-workspaces-2-new-tools-to-speed-kubernetes-development%2F&amp;#38;linkname=Red%20Hat%20CodeReady%20Workspaces%202%3A%20New%20tools%20to%20speed%20Kubernetes%20development" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F03%2Fred-hat-codeready-workspaces-2-new-tools-to-speed-kubernetes-development%2F&amp;#38;linkname=Red%20Hat%20CodeReady%20Workspaces%202%3A%20New%20tools%20to%20speed%20Kubernetes%20development" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F03%2Fred-hat-codeready-workspaces-2-new-tools-to-speed-kubernetes-development%2F&amp;#38;linkname=Red%20Hat%20CodeReady%20Workspaces%202%3A%20New%20tools%20to%20speed%20Kubernetes%20development" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F03%2Fred-hat-codeready-workspaces-2-new-tools-to-speed-kubernetes-development%2F&amp;#38;linkname=Red%20Hat%20CodeReady%20Workspaces%202%3A%20New%20tools%20to%20speed%20Kubernetes%20development" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F03%2Fred-hat-codeready-workspaces-2-new-tools-to-speed-kubernetes-development%2F&amp;#038;title=Red%20Hat%20CodeReady%20Workspaces%202%3A%20New%20tools%20to%20speed%20Kubernetes%20development" data-a2a-url="https://developers.redhat.com/blog/2019/12/03/red-hat-codeready-workspaces-2-new-tools-to-speed-kubernetes-development/" data-a2a-title="Red Hat CodeReady Workspaces 2: New tools to speed Kubernetes development"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/12/03/red-hat-codeready-workspaces-2-new-tools-to-speed-kubernetes-development/"&gt;Red Hat CodeReady Workspaces 2: New tools to speed Kubernetes development&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/1_gMQV7wJFo" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;We are pleased to announce the release of Red Hat CodeReady Workspaces 2.0. Based on Eclipse Che, its upstream project CodeReady Workspaces is a Red Hat OpenShift-native developer environment enabling cloud-native development for developer teams. CodeReady Workspaces 2.0 is available now on OpenShift 3.11 and OpenShift 4.x. This new version introduces: Kubernetes-native developer sandboxes on [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/12/03/red-hat-codeready-workspaces-2-new-tools-to-speed-kubernetes-development/"&gt;Red Hat CodeReady Workspaces 2: New tools to speed Kubernetes development&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><post-id xmlns="com-wordpress:feed-additions:1">660297</post-id><dc:creator>Stevan LeMeur</dc:creator><dc:date>2019-12-03T08:00:21Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/12/03/red-hat-codeready-workspaces-2-new-tools-to-speed-kubernetes-development/</feedburner:origLink></entry><entry><title>Testing in production: From DevTestOops to DevTestOps</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/E82EG5D4nNI/" /><category term="DevNation" /><category term="DevOps" /><category term="Events" /><category term="testing" /><author><name>Alex Soto Bueno</name></author><id>https://developers.redhat.com/blog/?p=657897</id><updated>2019-12-03T07:55:14Z</updated><published>2019-12-03T07:55:14Z</published><content type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/devnation/"&gt;DevNation tech talks&lt;/a&gt; are hosted by the Red Hat technologists who create our products. These sessions include real solutions and code and sample projects to help you get started. In this talk, you’ll learn about testing in production from Alex Soto, Red Hat Software Engineer.&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/topics/devops/"&gt;DevOps&lt;/a&gt; has grown in popularity in recent years, particularly in software companies that want to reduce lead time to be measured in days/weeks instead of months/years. To make sure your software does the right things and does those things right, you need to test it implacably. Many companies, however, see the testing phase as a bottleneck that slows product release. To change that, we need a new approach — making the release process of an application a testing process and involving QA from the beginning.&lt;/p&gt; &lt;p&gt;In this presentation, we will actively demonstrate several techniques that you can use immediately to start testing in production and speeding up your release cycle.&lt;span id="more-657897"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;Watch the complete presentation here:&lt;/p&gt; &lt;p&gt;&lt;iframe src="https://www.youtube.com/embed/EVD4EXj8RKc" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"&gt;&lt;/iframe&gt;&lt;/p&gt; &lt;h3&gt;Learn more&lt;/h3&gt; &lt;p&gt;Join us at an &lt;a href="https://developers.redhat.com/events/"&gt;upcoming developer event&lt;/a&gt;, and see our collection of past &lt;a href="https://developers.redhat.com/devnation/"&gt;DevNation tech talks&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F03%2Ftesting-in-production-from-devtestoops-to-devtestops%2F&amp;#38;linkname=Testing%20in%20production%3A%20From%20DevTestOops%20to%20DevTestOps" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F03%2Ftesting-in-production-from-devtestoops-to-devtestops%2F&amp;#38;linkname=Testing%20in%20production%3A%20From%20DevTestOops%20to%20DevTestOps" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F03%2Ftesting-in-production-from-devtestoops-to-devtestops%2F&amp;#38;linkname=Testing%20in%20production%3A%20From%20DevTestOops%20to%20DevTestOps" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F03%2Ftesting-in-production-from-devtestoops-to-devtestops%2F&amp;#38;linkname=Testing%20in%20production%3A%20From%20DevTestOops%20to%20DevTestOps" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F03%2Ftesting-in-production-from-devtestoops-to-devtestops%2F&amp;#38;linkname=Testing%20in%20production%3A%20From%20DevTestOops%20to%20DevTestOps" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F03%2Ftesting-in-production-from-devtestoops-to-devtestops%2F&amp;#38;linkname=Testing%20in%20production%3A%20From%20DevTestOops%20to%20DevTestOps" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F03%2Ftesting-in-production-from-devtestoops-to-devtestops%2F&amp;#38;linkname=Testing%20in%20production%3A%20From%20DevTestOops%20to%20DevTestOps" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2019%2F12%2F03%2Ftesting-in-production-from-devtestoops-to-devtestops%2F&amp;#038;title=Testing%20in%20production%3A%20From%20DevTestOops%20to%20DevTestOps" data-a2a-url="https://developers.redhat.com/blog/2019/12/03/testing-in-production-from-devtestoops-to-devtestops/" data-a2a-title="Testing in production: From DevTestOops to DevTestOps"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/12/03/testing-in-production-from-devtestoops-to-devtestops/"&gt;Testing in production: From DevTestOops to DevTestOps&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/E82EG5D4nNI" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;DevNation tech talks are hosted by the Red Hat technologists who create our products. These sessions include real solutions and code and sample projects to help you get started. In this talk, you’ll learn about testing in production from Alex Soto, Red Hat Software Engineer. DevOps has grown in popularity in recent years, particularly in [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2019/12/03/testing-in-production-from-devtestoops-to-devtestops/"&gt;Testing in production: From DevTestOops to DevTestOps&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;Red Hat Developer&lt;/a&gt;.&lt;/p&gt;</summary><post-id xmlns="com-wordpress:feed-additions:1">657897</post-id><dc:creator>Alex Soto Bueno</dc:creator><dc:date>2019-12-03T07:55:14Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2019/12/03/testing-in-production-from-devtestoops-to-devtestops/</feedburner:origLink></entry><entry><title>Apache Camel 3 - Top 10 of What's New</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/sv3hLK39ljg/apache-camel-3-top-10-of-whats-new.html" /><category term="apache camel" scheme="searchisko:content:tags" /><category term="feed_group_name_fusesource" scheme="searchisko:content:tags" /><category term="feed_name_clausibsen" scheme="searchisko:content:tags" /><category term="release" scheme="searchisko:content:tags" /><category term="roadmap" scheme="searchisko:content:tags" /><author><name>Claus Ibsen</name></author><id>searchisko:content:id:jbossorg_blog-apache_camel_3_top_10_of_what_s_new</id><updated>2019-12-02T19:09:02Z</updated><published>2019-12-02T19:09:00Z</published><content type="html">&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;Today I posted a &lt;a href="https://camel.apache.org/blog/Camel3-Whatsnew/"&gt;blog on the Apache Camel 3 website&lt;/a&gt; with a top 10 of what's new in the brand new Apache Camel v3 release.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-VvnBGXwbvyI/XeVhDRlpd2I/AAAAAAAACB8/PifRKJtZQOYwAI7F1au1cGOv4Xr5EcN_QCLcBGAsYHQ/s1600/camel3-3humps.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="496" data-original-width="578" height="274" src="https://1.bp.blogspot.com/-VvnBGXwbvyI/XeVhDRlpd2I/AAAAAAAACB8/PifRKJtZQOYwAI7F1au1cGOv4Xr5EcN_QCLcBGAsYHQ/s320/camel3-3humps.png" width="320" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;So head over to the &lt;a href="https://camel.apache.org/blog/Camel3-Whatsnew/"&gt;Camel website&lt;/a&gt; and read the blog post.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class="feedflare"&gt; &lt;a href="http://feeds.feedburner.com/~ff/ApacheCamel?a=U4a-OwbE808:-0MubjsAznc:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/ApacheCamel?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/ApacheCamel?a=U4a-OwbE808:-0MubjsAznc:4cEx4HpKnUU"&gt;&lt;img src="http://feeds.feedburner.com/~ff/ApacheCamel?i=U4a-OwbE808:-0MubjsAznc:4cEx4HpKnUU" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/ApacheCamel?a=U4a-OwbE808:-0MubjsAznc:F7zBnMyn0Lo"&gt;&lt;img src="http://feeds.feedburner.com/~ff/ApacheCamel?i=U4a-OwbE808:-0MubjsAznc:F7zBnMyn0Lo" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;a href="http://feeds.feedburner.com/~ff/ApacheCamel?a=U4a-OwbE808:-0MubjsAznc:V_sGLiPBpWU"&gt;&lt;img src="http://feeds.feedburner.com/~ff/ApacheCamel?i=U4a-OwbE808:-0MubjsAznc:V_sGLiPBpWU" border="0"&gt;&lt;/img&gt;&lt;/a&gt; &lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/ApacheCamel/~4/U4a-OwbE808" height="1" width="1" alt=""/&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/sv3hLK39ljg" height="1" width="1" alt=""/&gt;</content><summary>Today I posted a blog on the Apache Camel 3 website with a top 10 of what's new in the brand new Apache Camel v3 release. So head over to the Camel website and read the blog post.</summary><dc:creator>Claus Ibsen</dc:creator><dc:date>2019-12-02T19:09:00Z</dc:date><feedburner:origLink>http://feedproxy.google.com/~r/ApacheCamel/~3/U4a-OwbE808/apache-camel-3-top-10-of-whats-new.html</feedburner:origLink></entry></feed>
